{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "warnings.simplefilter('ignore')\n",
    "sys.path.append('/Users/nagataeiki/.pyenv/versions/3.7.4/lib/python3.7/site-packages')\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bad = pd.read_csv('../data/clean/cleaned_bad.csv')\n",
    "normal=pd.read_csv('../data/clean/cleaned_normal.csv')\n",
    "# hiniku_=pd.read_csv('../data/hiniku/hiniku_with_word.csv')\n",
    "hiniku = pd.read_csv(\"../data/clean/cleaned_hiniku.csv\")\n",
    "df = pd.read_csv('../data/clean/mount_all_data.csv')\n",
    "# df.drop('comment_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwordlist = pd.read_csv('../data/clean/mount_bad_list.csv')\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "badwordlist_reading = []\n",
    "for word  in badwordlist.bad.values:\n",
    "    text=\"\"\n",
    "    for t in tokenizer.tokenize(word):\n",
    "        if t.reading==\"*\":\n",
    "            pass\n",
    "        else:\n",
    "            text+=t.reading\n",
    "    if len(text)>=1:\n",
    "        badwordlist_reading.append(text)\n",
    "# pd.DataFrame(badwordlist_reading).to_csv('../data/clean/badwordlist_reading.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# badwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_and_pred(df, threshold=0.5):\n",
    "    df = df.copy()\n",
    "    df = df.dropna()\n",
    "    # testdf.target = testdf.target.apply(lambda x: 0 if x==0 else 1)#x==0(ニュートラル)の時0, その他は1\n",
    "    df.target = df.target.apply(lambda x: 1 if x==1 else 0)#x==1(悪口)の時1, その他は0 [0:悪口, 1:その他]\n",
    "    # testdf.target = testdf.target.apply(lambda x: 0 if x==2 else 1)#x==2(皮肉)の時0, その他は1\n",
    "    df.target.unique()\n",
    "    df.target.unique()\n",
    "    X = df.drop(\"target\",axis=1)#.drop('content', axis=1)\n",
    "    y=df.target\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)    \n",
    "\n",
    "    svm = SVR()\n",
    "    svm.fit(x_train.drop(['content'], axis=1), y_train)\n",
    "    pred_train_unadjust = svm.predict(x_train.drop(['content'], axis=1))\n",
    "    pred_test_unadjust = svm.predict(x_test.drop(['content'], axis=1))\n",
    "\n",
    "    print(\"train\")\n",
    "\n",
    "    pred_train = [1 if x>threshold else 0 for x in pred_train_unadjust]\n",
    "    pred_test = [1 if x>threshold else 0 for x in pred_test_unadjust]\n",
    "    print(\"f1: \" ,f1_score(y_train, pred_train))\n",
    "    display(create_2x2_matrix(y_train, pred_train))\n",
    "    # print_2x2_miss_pred(y_train, pred_train,x_train)\n",
    "    print('------------------------------------------------------------------')\n",
    "    print('test')\n",
    "\n",
    "    print(\"f1: \",f1_score(y_test, pred_test))\n",
    "    display(create_2x2_matrix(y_test, pred_test))\n",
    "    train_miss = print_2x2_miss_pred(true_y=y_train,pred_y=pred_train,df=x_train)\n",
    "    test_miss =  print_2x2_miss_pred(true_y=y_test, pred_y=pred_test,df=x_test)\n",
    "    return {\n",
    "        \"miss_train_df\":train_miss, \n",
    "        \"miss_test_df\":test_miss,\n",
    "        \"x_train\":x_train,\n",
    "        \"y_train\":y_train,\n",
    "        \"x_test\":x_test,\n",
    "        \"y_test\":y_test,\n",
    "        \"pred_train\":pred_train,\n",
    "        \"pred_test\":pred_test,\n",
    "        \"model\":svm,\n",
    "        \"pred_train_un\":pred_train_unadjust,\n",
    "        \"pred_test_un\":pred_test_unadjust\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2x2_matrix(true_y, pred_y):\n",
    "    matrix = np.zeros(4)\n",
    "\n",
    "    for t,p in zip(true_y, pred_y):\n",
    "        if t==0:\n",
    "            if p==0:matrix[0]+=1\n",
    "            if p==1:matrix[1]+=1\n",
    "        elif t==1:\n",
    "            if p==0:matrix[2]+=1\n",
    "            if p==1:matrix[3]+=1\n",
    "    return matrix.reshape(2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_2x2_miss_pred(true_y, pred_y,df, test=True):\n",
    "    def print_str_label(num):\n",
    "        if num==1:return \"悪口\"\n",
    "        if num==0:return \"皮肉or その他\"\n",
    "        return np.nan\n",
    "\n",
    "    idx_arr = []\n",
    "    pred_judge=[]\n",
    "    true_judge=[]\n",
    "    col=[\"content\", \"pred_idx\", \"pred_label\",\"true_idx\", \"true_label\"]\n",
    "    returndf = pd.DataFrame(columns=col)\n",
    "    for i in range(len(df)):\n",
    "        if pred_y[i]!=true_y.values[i]:\n",
    "            tmpdf = pd.DataFrame(np.array([df.iloc[i].content, pred_y[i], print_str_label(pred_y[i]),\\\n",
    "                                            true_y.values[i], print_str_label(true_y.values[i])]).reshape(1,-1), \\\n",
    "                                              columns=col)\n",
    "            returndf = pd.concat([returndf, tmpdf])\n",
    "    return returndf\n",
    "\n",
    "\n",
    "def print_2x2_correct_pred(true_y, pred_y,df, test=True):\n",
    "    def print_str_label(num):\n",
    "        if num==1:return \"悪口\"\n",
    "        if num==0:return \"皮肉or その他\"\n",
    "        return np.nan\n",
    "\n",
    "    idx_arr = []\n",
    "    pred_judge=[]\n",
    "    true_judge=[]\n",
    "    col=[\"content\", \"pred_idx\", \"pred_label\",\"true_idx\", \"true_label\"]\n",
    "    returndf = pd.DataFrame(columns=col)\n",
    "    for i in range(len(df)):\n",
    "        if pred_y[i]==true_y.values[i]:\n",
    "            tmpdf = pd.DataFrame(np.array([df.iloc[i].content, pred_y[i], print_str_label(pred_y[i]),\\\n",
    "                                            true_y.values[i], print_str_label(true_y.values[i])]).reshape(1,-1), \\\n",
    "                                              columns=col)\n",
    "            returndf = pd.concat([returndf, tmpdf])\n",
    "    return returndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wakati_content(text):\n",
    "    wakati = []\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        wakati.append(t.surface)\n",
    "    return wakati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "# # data = pd.read_csv('../data/clean/mount_bad_list.csv')\n",
    "# data = pd.read_csv(\"../data/clean/mount_all_data.csv\")\n",
    "# data = data[data.type==1]\n",
    "wakati_arr = []\n",
    "# for r in tqdm(data.iterrows()):\n",
    "#     wakati = create_wakati_content(r[1].content)\n",
    "#     wakati_arr.append(wakati)\n",
    "# model = Word2Vec(wakati_arr, sg=1, size=100, window=5, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('../data/some_comment_data.csv')\n",
    "\n",
    "# for r in tqdm(data.iterrows()):\n",
    "#     wakati = create_wakati_content(r[1].content)\n",
    "#     wakati_arr.append(wakati)\n",
    "# model = Word2Vec(wakati_arr, sg=1, size=100, window=5, min_count=1)\n",
    "# model.save(\"./word2vecModel/200000.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec.load(\"./word2vecModel/100000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1929it [00:22, 85.45it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>mophologics_num</th>\n",
       "      <th>bad_count_in_mophologic</th>\n",
       "      <th>simple_bad_flg</th>\n",
       "      <th>target</th>\n",
       "      <th>order</th>\n",
       "      <th>mophologic_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>なにこの糞ゲー</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[なに, この, 糞, ゲー]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ちょっと蛇足気味だな・・・</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[ちょっと, 蛇足, 気味, だ, な, ・, ・, ・]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         content  mophologics_num  bad_count_in_mophologic  simple_bad_flg  \\\n",
       "0        なにこの糞ゲー              4.0                      1.0               1   \n",
       "0  ちょっと蛇足気味だな・・・              8.0                      1.0               1   \n",
       "\n",
       "   target  order             mophologic_content  \n",
       "0       1      0                [なに, この, 糞, ゲー]  \n",
       "0       1      0  [ちょっと, 蛇足, 気味, だ, な, ・, ・, ・]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1929, 7)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "悪口ワードの含有\n",
    "皮肉ワードの含有\n",
    "ポジティブワードの含有\n",
    "\n",
    "[形態素解析した時の単語数],[単純に悪口単語が文章に含まれているのかのフラグ], [形態素ごとに現れる悪口数]\n",
    "[皮肉単語が含まれているかのフラグ],[形態素ごとに現れる皮肉数]\n",
    "[ポジティブな単語が含まれているかのフラグ], [形態素ごとに現れるポジティブ単語数]\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "# df = bad.copy()\n",
    "# df = pd.concat([hiniku_, normal])\n",
    "# df = pd.concat([bad, df])\n",
    "import sys\n",
    "import numpy as np\n",
    "from janome.tokenizer import Tokenizer\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "\n",
    "df = pd.read_csv('../data/clean/mount_all_data.csv')\n",
    "df.drop('comment_id', axis=1, inplace=True)\n",
    "tokenizer = Tokenizer()\n",
    "#悪口が含まれている数のカウント, 単に含まれているのかのフラグを返す\n",
    "def get_word_count(text, wordlist):\n",
    "    textlist = []\n",
    "    count=0\n",
    "    flg=0\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        if t.surface in wordlist:\n",
    "            count+=1\n",
    "    for w in wordlist:\n",
    "        if w in text:\n",
    "            flg=1\n",
    "            return count, flg\n",
    "    \n",
    "    return count,flg\n",
    "\n",
    "\n",
    "def get_order_label(text):\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        if t.infl_form[:2]==\"命令\":\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "'''\n",
    "###ここを実行することでword2vecのモデル作成\n",
    "from gensim.models import Word2Vec\n",
    "data = pd.read_csv('../data/some_comment_data.csv')\n",
    "newdata=pd.concat([pd.DataFrame(data.content), pd.DataFrame(df.content)])\n",
    "wakati_arr = []\n",
    "for r in tqdm(newdata.iterrows()):\n",
    "    wakati = create_wakati_content(r[1].content)\n",
    "    wakati_arr.append(wakadti)\n",
    "model = Word2Vec(wakati_arr, sg=1, size=100, window=5, min_count=1)\n",
    "'''\n",
    "\n",
    "def calc_cosine_similary(text,model,debug=False):\n",
    "    text_list = []\n",
    "    scores = {}\n",
    "\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        text_list.append(t.surface)\n",
    "    for word in text_list:\n",
    "        try:\n",
    "            vector1 = model.wv[word]\n",
    "            scores[word] = []\n",
    "            for bad in bad_list:\n",
    "                try:\n",
    "                    vector2 = model.wv[bad]\n",
    "                    score = cos_similarity(vector1,vector2)\n",
    "                    scores[word].append(score)\n",
    "                except :\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "    all_scores = []\n",
    "    for scores in scores.values():\n",
    "        all_scores +=scores\n",
    "    all_scores= np.array(all_scores)\n",
    "    typicalV = np.median(all_scores, axis=0)\n",
    "\n",
    "    if debug:\n",
    "        return scores\n",
    "    return typicalV\n",
    "\n",
    "\n",
    "def mophologicize(text):\n",
    "    words = []\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        words.append(t.surface)    \n",
    "    return words\n",
    "\n",
    "\n",
    "#dfと悪口辞書を入れて新しい特徴量を含んだdfを返す\n",
    "def make_features(df, badwordlist, positivelist=np.nan, hinikulist=np.nan):\n",
    "#     col=[\"content\",\"mophologics_num\", \"bad_per_mophologic\", \"simple_bad_flg\",\"target\", \"hiniku_per_mophologic\",\\\n",
    "#          \"simple_hiniku_flg\", \"positive_per_mophologic\", \"simple_positive_flg\"]\n",
    "#     import gensim\n",
    "#     PATH=\"./gensimmodel/word2vec.gensim.model\"\n",
    "#     model = gensim.models.Word2Vec.load(PATH)\n",
    "    col=[\"content\",\"mophologics_num\", \"bad_count_in_mophologic\", \"simple_bad_flg\",\"target\",\"order\",\"mophologic_content\"]\n",
    "\n",
    "    basedf = pd.DataFrame(columns=col)\n",
    "    tokenizer = Tokenizer()\n",
    "    for r in tqdm(df.iterrows()):\n",
    "        text = str(r[1].content)\n",
    "        target = r[1].type\n",
    "        num = len(tokenizer.tokenize(text))\n",
    "        badcount,badflg = get_word_count(text, badwordlist)\n",
    "        orderlabel = get_order_label(text)\n",
    "        mophologic = mophologicize(text)\n",
    "#         hinikucount, hinikuflg = get_word_count(text, hinikulist)\n",
    "#         positivecount, positiveflg = get_word_count(text, positivelist)\n",
    "#         newdf = pd.DataFrame(np.array([text, num, badcount,badflg,target,\\\n",
    "#                                        hinikucount, hinikuflg, positivecount,\\\n",
    "#                                        positiveflg]).reshape(1,-1),columns=col)\n",
    "        newdf = pd.DataFrame(np.array([text, num, badcount,badflg,target, orderlabel,mophologic]).reshape(1,-1),columns=col)\n",
    "        basedf = pd.concat([basedf, newdf])\n",
    "\n",
    "    basedf[\"target\"] = basedf.apply(lambda x: int(x[\"target\"]), axis=1)\n",
    "#     basedf[\"comment_id\"] = basedf.index\n",
    "    basedf[\"bad_count_in_mophologic\"] = basedf.apply(lambda x: float(x[\"bad_count_in_mophologic\"]), axis=1)\n",
    "    basedf[\"mophologics_num\"] = basedf.apply(lambda x: float(x[\"mophologics_num\"]), axis=1)\n",
    "    basedf[\"order\"] = basedf.order.apply(lambda x: int(x))\n",
    "#     basedf[\"cosine_similar\"] = basedf.cosine_similar.apply(lambda x: float(x))\n",
    "    \n",
    "    return basedf\n",
    "df =make_features(df, list(badwordlist.bad.values))\n",
    "df = df.fillna(0.0)\n",
    "display(df.head(2))\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bad_word_list():\n",
    "    b_l=[]\n",
    "    #wor2vecの辞書にない単語の除外\n",
    "    for word in badwordlist.bad.values:\n",
    "        try:\n",
    "            a = model.wv[word]\n",
    "            b_l.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    return b_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1929it [00:05, 345.44it/s]\n"
     ]
    }
   ],
   "source": [
    "col=[]\n",
    "for i in range(100):\n",
    "    col.append(\"f_\"+str(i))\n",
    "\n",
    "# def create_mean_vector_data(df):\n",
    "\n",
    "sandbox=pd.DataFrame(columns=col)\n",
    "for r in tqdm(df.iterrows()):\n",
    "    text = r[1].mophologic_content\n",
    "    vec_list=[]\n",
    "    for t in text:\n",
    "            try:\n",
    "                vec = model.wv[t]\n",
    "                vec_list.append(vec)\n",
    "            except:\n",
    "                vec_list.append(np.zeros(100))\n",
    "    vectors = pd.DataFrame(np.array(vec_list), columns=col)\n",
    "    tmp = pd.DataFrame(pd.DataFrame(vectors, columns=col).mean(axis=0)).T\n",
    "    sandbox = pd.concat([sandbox, tmp])\n",
    "        \n",
    "sandbox.insert(0, \"target\",df.target)\n",
    "sandbox.insert(1,\"content\",df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_90</th>\n",
       "      <th>f_91</th>\n",
       "      <th>f_92</th>\n",
       "      <th>f_93</th>\n",
       "      <th>f_94</th>\n",
       "      <th>f_95</th>\n",
       "      <th>f_96</th>\n",
       "      <th>f_97</th>\n",
       "      <th>f_98</th>\n",
       "      <th>f_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>新キャラはいないのね</td>\n",
       "      <td>0.202817</td>\n",
       "      <td>-0.047081</td>\n",
       "      <td>-0.185330</td>\n",
       "      <td>-0.138851</td>\n",
       "      <td>0.080809</td>\n",
       "      <td>0.171307</td>\n",
       "      <td>-0.183035</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>-0.302942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078285</td>\n",
       "      <td>-0.203599</td>\n",
       "      <td>0.405191</td>\n",
       "      <td>0.036365</td>\n",
       "      <td>0.023481</td>\n",
       "      <td>0.021997</td>\n",
       "      <td>0.123947</td>\n",
       "      <td>0.472234</td>\n",
       "      <td>-0.079104</td>\n",
       "      <td>0.051222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>結局皆愚民</td>\n",
       "      <td>0.089381</td>\n",
       "      <td>0.049561</td>\n",
       "      <td>-0.184039</td>\n",
       "      <td>-0.021396</td>\n",
       "      <td>0.036245</td>\n",
       "      <td>0.195033</td>\n",
       "      <td>-0.106482</td>\n",
       "      <td>0.060902</td>\n",
       "      <td>-0.208542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109901</td>\n",
       "      <td>-0.139076</td>\n",
       "      <td>0.216027</td>\n",
       "      <td>0.021628</td>\n",
       "      <td>0.106497</td>\n",
       "      <td>-0.077331</td>\n",
       "      <td>0.079978</td>\n",
       "      <td>0.222107</td>\n",
       "      <td>-0.082260</td>\n",
       "      <td>-0.057868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蛇足は同意</td>\n",
       "      <td>0.208156</td>\n",
       "      <td>0.006126</td>\n",
       "      <td>-0.132161</td>\n",
       "      <td>-0.044880</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>-0.085165</td>\n",
       "      <td>0.097970</td>\n",
       "      <td>-0.079012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037020</td>\n",
       "      <td>-0.047831</td>\n",
       "      <td>0.301364</td>\n",
       "      <td>0.015739</td>\n",
       "      <td>0.187576</td>\n",
       "      <td>0.087266</td>\n",
       "      <td>0.031497</td>\n",
       "      <td>0.311311</td>\n",
       "      <td>-0.030625</td>\n",
       "      <td>-0.046450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>この開発者ほんとうに殺してえｗｗｗｗ</td>\n",
       "      <td>0.175828</td>\n",
       "      <td>0.079850</td>\n",
       "      <td>-0.161196</td>\n",
       "      <td>-0.157549</td>\n",
       "      <td>-0.020126</td>\n",
       "      <td>0.118874</td>\n",
       "      <td>-0.163840</td>\n",
       "      <td>0.097599</td>\n",
       "      <td>-0.155239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165546</td>\n",
       "      <td>-0.101738</td>\n",
       "      <td>0.419856</td>\n",
       "      <td>-0.030870</td>\n",
       "      <td>0.093836</td>\n",
       "      <td>-0.071714</td>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.313291</td>\n",
       "      <td>-0.218188</td>\n",
       "      <td>-0.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>すぐ似合います</td>\n",
       "      <td>0.363841</td>\n",
       "      <td>0.097825</td>\n",
       "      <td>-0.553699</td>\n",
       "      <td>-0.251060</td>\n",
       "      <td>0.044208</td>\n",
       "      <td>-0.020454</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>0.315776</td>\n",
       "      <td>-0.178766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138611</td>\n",
       "      <td>-0.309578</td>\n",
       "      <td>0.404622</td>\n",
       "      <td>-0.065032</td>\n",
       "      <td>0.162018</td>\n",
       "      <td>-0.065732</td>\n",
       "      <td>0.306731</td>\n",
       "      <td>0.248009</td>\n",
       "      <td>-0.141145</td>\n",
       "      <td>-0.245515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>あなたの服のセンスって流行に流されずに個性的ですね。</td>\n",
       "      <td>0.085914</td>\n",
       "      <td>0.050023</td>\n",
       "      <td>-0.253516</td>\n",
       "      <td>-0.147846</td>\n",
       "      <td>0.091852</td>\n",
       "      <td>0.204806</td>\n",
       "      <td>-0.214809</td>\n",
       "      <td>0.085059</td>\n",
       "      <td>-0.307011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083019</td>\n",
       "      <td>-0.201691</td>\n",
       "      <td>0.362291</td>\n",
       "      <td>-0.024693</td>\n",
       "      <td>0.058607</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.214603</td>\n",
       "      <td>0.296678</td>\n",
       "      <td>-0.196848</td>\n",
       "      <td>-0.085599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>しょ、しょうがない</td>\n",
       "      <td>0.201207</td>\n",
       "      <td>-0.130418</td>\n",
       "      <td>-0.106762</td>\n",
       "      <td>0.040645</td>\n",
       "      <td>0.105814</td>\n",
       "      <td>0.162713</td>\n",
       "      <td>-0.142285</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>-0.361920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138408</td>\n",
       "      <td>-0.191072</td>\n",
       "      <td>0.390690</td>\n",
       "      <td>0.030164</td>\n",
       "      <td>0.086039</td>\n",
       "      <td>-0.040707</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.339462</td>\n",
       "      <td>-0.145489</td>\n",
       "      <td>0.092378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>高校生だからこんなに頭悪いのか</td>\n",
       "      <td>0.067208</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>-0.110589</td>\n",
       "      <td>-0.138313</td>\n",
       "      <td>0.023983</td>\n",
       "      <td>0.257455</td>\n",
       "      <td>-0.172598</td>\n",
       "      <td>0.163545</td>\n",
       "      <td>-0.356657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>-0.082248</td>\n",
       "      <td>0.358481</td>\n",
       "      <td>0.082923</td>\n",
       "      <td>0.130186</td>\n",
       "      <td>-0.196028</td>\n",
       "      <td>0.175964</td>\n",
       "      <td>0.376834</td>\n",
       "      <td>-0.277162</td>\n",
       "      <td>-0.141150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>携帯（通話のみ）だなｗ　教養１が</td>\n",
       "      <td>0.186904</td>\n",
       "      <td>-0.192314</td>\n",
       "      <td>-0.075863</td>\n",
       "      <td>-0.126739</td>\n",
       "      <td>0.050477</td>\n",
       "      <td>0.262073</td>\n",
       "      <td>-0.116341</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>-0.134683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096489</td>\n",
       "      <td>-0.273611</td>\n",
       "      <td>0.232606</td>\n",
       "      <td>0.042030</td>\n",
       "      <td>0.116653</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.193589</td>\n",
       "      <td>0.314831</td>\n",
       "      <td>-0.168358</td>\n",
       "      <td>-0.173684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>前奏長いし、ダレてる。再構築っつーかグチャグチャにしただけって感じ</td>\n",
       "      <td>0.152653</td>\n",
       "      <td>0.018151</td>\n",
       "      <td>-0.266462</td>\n",
       "      <td>-0.151262</td>\n",
       "      <td>0.010838</td>\n",
       "      <td>0.208701</td>\n",
       "      <td>-0.054780</td>\n",
       "      <td>0.012604</td>\n",
       "      <td>-0.270590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228855</td>\n",
       "      <td>-0.212040</td>\n",
       "      <td>0.253001</td>\n",
       "      <td>-0.086128</td>\n",
       "      <td>0.042880</td>\n",
       "      <td>0.032681</td>\n",
       "      <td>0.114577</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>-0.244269</td>\n",
       "      <td>-0.097690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1543 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              content       f_0       f_1       f_2       f_3  \\\n",
       "0                          新キャラはいないのね  0.202817 -0.047081 -0.185330 -0.138851   \n",
       "0                               結局皆愚民  0.089381  0.049561 -0.184039 -0.021396   \n",
       "0                               蛇足は同意  0.208156  0.006126 -0.132161 -0.044880   \n",
       "0                  この開発者ほんとうに殺してえｗｗｗｗ  0.175828  0.079850 -0.161196 -0.157549   \n",
       "0                             すぐ似合います  0.363841  0.097825 -0.553699 -0.251060   \n",
       "..                                ...       ...       ...       ...       ...   \n",
       "0          あなたの服のセンスって流行に流されずに個性的ですね。  0.085914  0.050023 -0.253516 -0.147846   \n",
       "0                           しょ、しょうがない  0.201207 -0.130418 -0.106762  0.040645   \n",
       "0                     高校生だからこんなに頭悪いのか  0.067208 -0.002582 -0.110589 -0.138313   \n",
       "0                    携帯（通話のみ）だなｗ　教養１が  0.186904 -0.192314 -0.075863 -0.126739   \n",
       "0   前奏長いし、ダレてる。再構築っつーかグチャグチャにしただけって感じ  0.152653  0.018151 -0.266462 -0.151262   \n",
       "\n",
       "         f_4       f_5       f_6       f_7       f_8  ...      f_90      f_91  \\\n",
       "0   0.080809  0.171307 -0.183035  0.010823 -0.302942  ... -0.078285 -0.203599   \n",
       "0   0.036245  0.195033 -0.106482  0.060902 -0.208542  ... -0.109901 -0.139076   \n",
       "0   0.013240  0.201669 -0.085165  0.097970 -0.079012  ...  0.037020 -0.047831   \n",
       "0  -0.020126  0.118874 -0.163840  0.097599 -0.155239  ... -0.165546 -0.101738   \n",
       "0   0.044208 -0.020454 -0.050287  0.315776 -0.178766  ... -0.138611 -0.309578   \n",
       "..       ...       ...       ...       ...       ...  ...       ...       ...   \n",
       "0   0.091852  0.204806 -0.214809  0.085059 -0.307011  ... -0.083019 -0.201691   \n",
       "0   0.105814  0.162713 -0.142285  0.013064 -0.361920  ... -0.138408 -0.191072   \n",
       "0   0.023983  0.257455 -0.172598  0.163545 -0.356657  ...  0.000815 -0.082248   \n",
       "0   0.050477  0.262073 -0.116341  0.018088 -0.134683  ... -0.096489 -0.273611   \n",
       "0   0.010838  0.208701 -0.054780  0.012604 -0.270590  ... -0.228855 -0.212040   \n",
       "\n",
       "        f_92      f_93      f_94      f_95      f_96      f_97      f_98  \\\n",
       "0   0.405191  0.036365  0.023481  0.021997  0.123947  0.472234 -0.079104   \n",
       "0   0.216027  0.021628  0.106497 -0.077331  0.079978  0.222107 -0.082260   \n",
       "0   0.301364  0.015739  0.187576  0.087266  0.031497  0.311311 -0.030625   \n",
       "0   0.419856 -0.030870  0.093836 -0.071714  0.211921  0.313291 -0.218188   \n",
       "0   0.404622 -0.065032  0.162018 -0.065732  0.306731  0.248009 -0.141145   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "0   0.362291 -0.024693  0.058607 -0.000597  0.214603  0.296678 -0.196848   \n",
       "0   0.390690  0.030164  0.086039 -0.040707  0.126785  0.339462 -0.145489   \n",
       "0   0.358481  0.082923  0.130186 -0.196028  0.175964  0.376834 -0.277162   \n",
       "0   0.232606  0.042030  0.116653  0.003236  0.193589  0.314831 -0.168358   \n",
       "0   0.253001 -0.086128  0.042880  0.032681  0.114577  0.189873 -0.244269   \n",
       "\n",
       "        f_99  \n",
       "0   0.051222  \n",
       "0  -0.057868  \n",
       "0  -0.046450  \n",
       "0  -0.135400  \n",
       "0  -0.245515  \n",
       "..       ...  \n",
       "0  -0.085599  \n",
       "0   0.092378  \n",
       "0  -0.141150  \n",
       "0  -0.173684  \n",
       "0  -0.097690  \n",
       "\n",
       "[1543 rows x 101 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1=train_and_pred(sandbox,threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1929it [00:07, 253.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# def choice_the_highest_cosine_sim_vector_data(df):\n",
    "\n",
    "sandbox=pd.DataFrame(columns=col)\n",
    "for r in tqdm(df.iterrows()):\n",
    "    text = r[1].mophologic_content\n",
    "    vec_list=[]\n",
    "    flg=False\n",
    "    for t in text:#テキストの形態素ごとにループ\n",
    "            try:\n",
    "                vec = model.wv[t]\n",
    "                vec_list.append(vec)\n",
    "            except:\n",
    "                vec_list.append(np.zeros(100))\n",
    "    if flg:\n",
    "        pass\n",
    "    else:\n",
    "        vec_list = pd.DataFrame(np.array(vec_list), columns=col)\n",
    "\n",
    "        tmp = pd.DataFrame(pd.DataFrame(vec_list, columns=col).mean(axis=0)).T\n",
    "        sandbox = pd.concat([sandbox, tmp])\n",
    "        \n",
    "sandbox.insert(0, \"target\",df.target)\n",
    "sandbox.insert(1,\"content\",df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_2x2_miss_pred(true_y=res[\"y_test\"],pred_y=res[\"pred_test\"],df=res[\"x_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_2x2_correct_pred(true_y=res[\"y_test\"],pred_y=res[\"pred_test\"],df=res[\"x_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_cosine_sim_data(df):\n",
    "    b_l=create_bad_word_list()\n",
    "    base = pd.DataFrame(columns=np.array(b_l))\n",
    "    \n",
    "    for r in tqdm(df.iterrows()):\n",
    "        text = r[1].mophologic_content\n",
    "        ave_list=[]\n",
    "\n",
    "        for word in b_l:\n",
    "            try:\n",
    "                v1 = model.wv[word]\n",
    "                cos_list = []\n",
    "                for t in text:\n",
    "                        v2=model.wv[t]\n",
    "                        cos_list.append(cos_similarity(v1,v2))\n",
    "                        if word not in b_l:\n",
    "                            b_l.append(word)\n",
    "            except:\n",
    "                pass\n",
    "            ave_list.append(np.array(cos_list).mean())\n",
    "        tmp=pd.DataFrame(np.array(ave_list).reshape(1,-1), columns=b_l)\n",
    "        base=pd.concat([base, tmp])\n",
    "        \n",
    "    base[\"target\"] =df.target\n",
    "    base[\"content\"] = df.content\n",
    "    base = base.loc[:,base.columns[::-1]]\n",
    "\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1929it [01:11, 26.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>はげ</th>\n",
       "      <th>キモオタ</th>\n",
       "      <th>バカー</th>\n",
       "      <th>きしょい</th>\n",
       "      <th>クソゲー</th>\n",
       "      <th>クソムシ</th>\n",
       "      <th>きらい</th>\n",
       "      <th>gm</th>\n",
       "      <th>...</th>\n",
       "      <th>シカト</th>\n",
       "      <th>ゆとり</th>\n",
       "      <th>ダサい</th>\n",
       "      <th>死ね</th>\n",
       "      <th>KY</th>\n",
       "      <th>イタイ</th>\n",
       "      <th>ばっか</th>\n",
       "      <th>ヘタ</th>\n",
       "      <th>蛇足</th>\n",
       "      <th>糞</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>なにこの糞ゲー</td>\n",
       "      <td>1</td>\n",
       "      <td>0.737712</td>\n",
       "      <td>0.585149</td>\n",
       "      <td>0.553066</td>\n",
       "      <td>0.503287</td>\n",
       "      <td>0.660441</td>\n",
       "      <td>0.685455</td>\n",
       "      <td>0.790680</td>\n",
       "      <td>0.616264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.808434</td>\n",
       "      <td>0.770303</td>\n",
       "      <td>0.681981</td>\n",
       "      <td>0.660041</td>\n",
       "      <td>0.673967</td>\n",
       "      <td>0.833161</td>\n",
       "      <td>0.694381</td>\n",
       "      <td>0.698025</td>\n",
       "      <td>0.844980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ちょっと蛇足気味だな・・・</td>\n",
       "      <td>1</td>\n",
       "      <td>0.670004</td>\n",
       "      <td>0.588808</td>\n",
       "      <td>0.553636</td>\n",
       "      <td>0.499966</td>\n",
       "      <td>0.630143</td>\n",
       "      <td>0.643968</td>\n",
       "      <td>0.664984</td>\n",
       "      <td>0.602853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588668</td>\n",
       "      <td>0.682783</td>\n",
       "      <td>0.706425</td>\n",
       "      <td>0.545869</td>\n",
       "      <td>0.592983</td>\n",
       "      <td>0.637162</td>\n",
       "      <td>0.679401</td>\n",
       "      <td>0.664682</td>\n",
       "      <td>0.654728</td>\n",
       "      <td>0.703167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蛇足は同意</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833095</td>\n",
       "      <td>0.794032</td>\n",
       "      <td>0.705397</td>\n",
       "      <td>0.689864</td>\n",
       "      <td>0.834422</td>\n",
       "      <td>0.844973</td>\n",
       "      <td>0.823597</td>\n",
       "      <td>0.822670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774778</td>\n",
       "      <td>0.816561</td>\n",
       "      <td>0.865877</td>\n",
       "      <td>0.677959</td>\n",
       "      <td>0.799566</td>\n",
       "      <td>0.823735</td>\n",
       "      <td>0.757569</td>\n",
       "      <td>0.806733</td>\n",
       "      <td>0.857879</td>\n",
       "      <td>0.779104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         content  target        はげ      キモオタ       バカー      きしょい      クソゲー  \\\n",
       "0        なにこの糞ゲー       1  0.737712  0.585149  0.553066  0.503287  0.660441   \n",
       "0  ちょっと蛇足気味だな・・・       1  0.670004  0.588808  0.553636  0.499966  0.630143   \n",
       "0          蛇足は同意       1  0.833095  0.794032  0.705397  0.689864  0.834422   \n",
       "\n",
       "       クソムシ       きらい        gm  ...       シカト       ゆとり       ダサい        死ね  \\\n",
       "0  0.685455  0.790680  0.616264  ...  0.596774  0.808434  0.770303  0.681981   \n",
       "0  0.643968  0.664984  0.602853  ...  0.588668  0.682783  0.706425  0.545869   \n",
       "0  0.844973  0.823597  0.822670  ...  0.774778  0.816561  0.865877  0.677959   \n",
       "\n",
       "         KY       イタイ       ばっか        ヘタ        蛇足         糞  \n",
       "0  0.660041  0.673967  0.833161  0.694381  0.698025  0.844980  \n",
       "0  0.592983  0.637162  0.679401  0.664682  0.654728  0.703167  \n",
       "0  0.799566  0.823735  0.757569  0.806733  0.857879  0.779104  \n",
       "\n",
       "[3 rows x 154 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sim_df = create_mean_cosine_sim_data(df)\n",
    "mean_sim_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "f1:  0.7031914893617021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[273., 481.],\n",
       "       [ 77., 661.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "test\n",
      "f1:  0.6967213114754097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 56., 124.],\n",
       "       [ 24., 170.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res2 = train_and_pred(mean_sim_df,threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "f1:  0.6954365079365079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[177., 577.],\n",
       "       [ 37., 701.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "test\n",
      "f1:  0.7047619047619048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 34., 146.],\n",
       "       [  9., 185.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res2 = train_and_pred(mean_sim_df,threshold=0.12)#model100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 = model.wv[\"はげ\"]\n",
    "# for t in text:\n",
    "#     v2 = model.wv[t]\n",
    "#     print(t, cos_similarity(v1,v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1929it [01:00, 31.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>はげ</th>\n",
       "      <th>キモオタ</th>\n",
       "      <th>バカー</th>\n",
       "      <th>きしょい</th>\n",
       "      <th>クソゲー</th>\n",
       "      <th>クソムシ</th>\n",
       "      <th>きらい</th>\n",
       "      <th>gm</th>\n",
       "      <th>...</th>\n",
       "      <th>シカト</th>\n",
       "      <th>ゆとり</th>\n",
       "      <th>ダサい</th>\n",
       "      <th>死ね</th>\n",
       "      <th>KY</th>\n",
       "      <th>イタイ</th>\n",
       "      <th>ばっか</th>\n",
       "      <th>ヘタ</th>\n",
       "      <th>蛇足</th>\n",
       "      <th>糞</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>なにこの糞ゲー</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865687</td>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.690638</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>0.795038</td>\n",
       "      <td>0.815677</td>\n",
       "      <td>0.896863</td>\n",
       "      <td>0.726885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704689</td>\n",
       "      <td>0.929999</td>\n",
       "      <td>0.882585</td>\n",
       "      <td>0.820817</td>\n",
       "      <td>0.778329</td>\n",
       "      <td>0.792534</td>\n",
       "      <td>0.958242</td>\n",
       "      <td>0.817534</td>\n",
       "      <td>0.830522</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ちょっと蛇足気味だな・・・</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946054</td>\n",
       "      <td>0.952036</td>\n",
       "      <td>0.852232</td>\n",
       "      <td>0.798920</td>\n",
       "      <td>0.979366</td>\n",
       "      <td>0.966861</td>\n",
       "      <td>0.911843</td>\n",
       "      <td>0.970934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893810</td>\n",
       "      <td>0.919892</td>\n",
       "      <td>0.976364</td>\n",
       "      <td>0.761640</td>\n",
       "      <td>0.930671</td>\n",
       "      <td>0.938139</td>\n",
       "      <td>0.865346</td>\n",
       "      <td>0.919833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         content  target        はげ      キモオタ       バカー      きしょい      クソゲー  \\\n",
       "0        なにこの糞ゲー       1  0.865687  0.704362  0.690638  0.628814  0.795038   \n",
       "0  ちょっと蛇足気味だな・・・       1  0.946054  0.952036  0.852232  0.798920  0.979366   \n",
       "\n",
       "       クソムシ       きらい        gm  ...       シカト       ゆとり       ダサい        死ね  \\\n",
       "0  0.815677  0.896863  0.726885  ...  0.704689  0.929999  0.882585  0.820817   \n",
       "0  0.966861  0.911843  0.970934  ...  0.893810  0.919892  0.976364  0.761640   \n",
       "\n",
       "         KY       イタイ       ばっか        ヘタ        蛇足         糞  \n",
       "0  0.778329  0.792534  0.958242  0.817534  0.830522  1.000000  \n",
       "0  0.930671  0.938139  0.865346  0.919833  1.000000  0.886566  \n",
       "\n",
       "[2 rows x 154 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_the_highest_cosine_sim_data(df):\n",
    "    b_l = create_bad_word_list()\n",
    "    base = pd.DataFrame(columns=np.array(b_l))\n",
    "    \n",
    "    for r in tqdm(df.iterrows()):\n",
    "        text = r[1].mophologic_content\n",
    "        highest_list=[]\n",
    "\n",
    "        for word in b_l:\n",
    "            try:\n",
    "                v1 = model.wv[word]\n",
    "                cos_list = []\n",
    "                for t in text:\n",
    "                        v2=model.wv[t]\n",
    "                        cos_list.append(cos_similarity(v1,v2))\n",
    "                        if word not in b_l:\n",
    "                            b_l.append(word)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                highest_list.append(np.array(cos_list).max())\n",
    "            except:\n",
    "                highest_list.append(np.nan)\n",
    "        tmp=pd.DataFrame(np.array(highest_list).reshape(1,-1), columns=b_l)\n",
    "        base=pd.concat([base, tmp])\n",
    "        \n",
    "    base[\"target\"] =df.target\n",
    "    base[\"content\"] = df.content\n",
    "    base = base.loc[:,base.columns[::-1]]\n",
    "    return base\n",
    "\n",
    "high_sim_df = create_the_highest_cosine_sim_data(df)\n",
    "high_sim_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "f1:  0.7386489479512736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[353., 401.],\n",
       "       [ 71., 667.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "test\n",
      "f1:  0.7337526205450734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 72., 108.],\n",
       "       [ 19., 175.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res3 = train_and_pred(high_sim_df,threshold=0.26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "f1:  0.7386489479512736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[353., 401.],\n",
       "       [ 71., 667.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "test\n",
      "f1:  0.7337526205450734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 72., 108.],\n",
       "       [ 19., 175.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res3 = train_and_pred(high_sim_df,threshold=0.26)#model100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdf = base.copy()\n",
    "# testdf = testdf.dropna()\n",
    "# # testdf.target = testdf.target.apply(lambda x: 0 if x==0 else 1)#x==0(ニュートラル)の時0, その他は1\n",
    "# testdf.target = testdf.target.apply(lambda x: 0 if x==1 else 1)#x==1(悪口)の時0, その他は1 [0:悪口, 1:その他]\n",
    "# # testdf.target = testdf.target.apply(lambda x: 0 if x==2 else 1)#x==2(皮肉)の時0, その他は1\n",
    "# testdf.target.unique()\n",
    "# testdf.target.unique()\n",
    "# X = testdf.drop(\"target\",axis=1)#.drop('content', axis=1)\n",
    "# y=testdf.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 1対1で予測をする時\n",
    "# '''\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# def vectorize(x_train,x_test, vec):\n",
    "#     x_train_vec = vec.fit_transform(x_train.content)\n",
    "#     x_test_vec = vec.transform(x_test.content)\n",
    "    \n",
    "#     x_train_vec = pd.DataFrame(x_train_vec.todense())\n",
    "#     x_test_vec = pd.DataFrame(x_test_vec.todense())\n",
    "#     return x_train_vec,x_test_vec\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "def train_and_pred(df, threshold=0.5):\n",
    "    df = df.copy()\n",
    "    df = df.dropna()\n",
    "    # testdf.target = testdf.target.apply(lambda x: 0 if x==0 else 1)#x==0(ニュートラル)の時0, その他は1\n",
    "    df.target = df.target.apply(lambda x: 1 if x==1 else 0)#x==1(悪口)の時1, その他は0 [0:悪口, 1:その他]\n",
    "    # testdf.target = testdf.target.apply(lambda x: 0 if x==2 else 1)#x==2(皮肉)の時0, その他は1\n",
    "    df.target.unique()\n",
    "    df.target.unique()\n",
    "    X = df.drop(\"target\",axis=1)#.drop('content', axis=1)\n",
    "    y=df.target\n",
    "\n",
    "    \n",
    "    svm = SVR()\n",
    "    svm.fit(x_train.drop(['content'], axis=1), y_train)\n",
    "    pred_train = svm.predict(x_train.drop(['content'], axis=1))\n",
    "    pred_test = svm.predict(x_test.drop(['content'], axis=1))\n",
    "\n",
    "    print(\"train\")\n",
    "\n",
    "    pred_train = [1 if x>threshold else 0 for x in pred_train]\n",
    "    pred_test = [1 if x>threshold else 0 for x in pred_test]\n",
    "    print(\"f1: \" ,f1_score(y_train, pred_train))\n",
    "    display(create_2x2_matrix(y_train, pred_train))\n",
    "    # print_2x2_miss_pred(y_train, pred_train,x_train)\n",
    "    print('------------------------------------------------------------------')\n",
    "    print('test')\n",
    "\n",
    "    print(\"f1: \",f1_score(y_test, pred_test))\n",
    "    display(create_2x2_matrix(y_test, pred_test))\n",
    "    train_miss = print_2x2_miss_pred(y_train, pred_train,x_train)\n",
    "    test_miss =  print_2x2_miss_pred(true_y=y_test, pred_y=pred_test,df=x_test)\n",
    "    return {\n",
    "        \"miss_train_df\":train_miss, \n",
    "        \"miss_test_df\":test_miss,\n",
    "        \"x_train\":x_train,\n",
    "        \"y_train\":y_train,\n",
    "        \"x_test\":x_test,\n",
    "        \"y_test\":y_test,\n",
    "        \"pred_train\":pred_train,\n",
    "        \"pred_test\":pred_test\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVR\n",
    "# svm = SVR()\n",
    "\n",
    "# # svm.fit(x_train_vec, y_train)\n",
    "# # pred_train = svm.predict(x_train_vec)\n",
    "# # pred_test = svm.predict(x_test_vec)\n",
    "\n",
    "\n",
    "# # svm.fit(x_train.drop(['content',\"cosine_similary\"], axis=1), y_train)\n",
    "# # pred_train = svm.predict(x_train.drop(['content',\"cosine_similary\"], axis=1))\n",
    "# # pred_test = svm.predict(x_test.drop(['content',\"cosine_similary\"], axis=1))\n",
    "\n",
    "\n",
    "# svm.fit(x_train.drop(['content'], axis=1), y_train)\n",
    "# pred_train = svm.predict(x_train.drop(['content'], axis=1))\n",
    "# pred_test = svm.predict(x_test.drop(['content'], axis=1))\n",
    "\n",
    "# print(\"train\")\n",
    "\n",
    "# pred_train = [1 if x>0.899 else 0 for x in pred_train]\n",
    "# pred_test = [1 if x>0.899 else 0 for x in pred_test]\n",
    "# print(f1_score(y_train, pred_train))\n",
    "# display(create_2x2_matrix(y_train, pred_train))\n",
    "# # print_2x2_miss_pred(y_train, pred_train,x_train)\n",
    "# print('------------------------------------------------------------------')\n",
    "# print('test')\n",
    "\n",
    "# print(f1_score(y_test, pred_test))\n",
    "# display(create_2x2_matrix(y_test, pred_test))\n",
    "# # print_2x2_miss_pred(true_y=y_test, pred_y=pred_test,df=x_test).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_miss = print_2x2_miss_pred(y_train, pred_train,x_train)\n",
    "# test_miss =  print_2x2_miss_pred(true_y=y_test, pred_y=pred_test,df=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pred_df = print_2x2_pred_true(y_train, pred_train,x_train)\n",
    "# test_pred_df =  print_2x2_pred_true(true_y=y_test, pred_y=pred_test,df=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "gbm = lgb.LGBMClassifier()\n",
    "# x_train.simple_bad_flg = x_train.simple_bad_flg.apply(lambda x: int(x))\n",
    "# x_test.simple_bad_flg = x_test.simple_bad_flg.apply(lambda x: int(x))\n",
    "# x_test.cosine_similar = x_test.cosine_similar.apply(lambda x: float(x))\n",
    "\n",
    "x_train = res3[\"x_train\"]\n",
    "y_train = res3[\"y_train\"]\n",
    "x_test = res3[\"x_test\"]\n",
    "y_test = res3[\"y_test\"]\n",
    "\n",
    "gbm.fit(x_train.drop('content',axis=1),y_train)\n",
    "pred_train = gbm.predict(x_train.drop('content',axis=1))\n",
    "pred_test = gbm.predict(x_test.drop('content',axis=1))\n",
    "\n",
    "# gbm.fit(x_train.drop(['content',\"cosine_similary\"],axis=1),y_train)\n",
    "# pred_train = gbm.predict(x_train.drop(['content',\"cosine_similary\"],axis=1))\n",
    "# pred_test = gbm.predict(x_test.drop(['content',\"cosine_similary\"],axis=1))\n",
    "'''\n",
    "train\n",
    "0.8698839340256568\n",
    "array([[618., 130.],\n",
    "       [ 83., 712.]])\n",
    "test\n",
    "0.919315403422983\n",
    "array([[165.,  19.],\n",
    "       [ 14., 188.]])\n",
    "'''\n",
    "\n",
    "# gbm.fit(x_train_vec, y_train)\n",
    "# pred_train = gbm.predict(x_train_vec)\n",
    "# pred_test = gbm.predict(x_test_vec)\n",
    "\n",
    "\n",
    "print(\"train\")\n",
    "print(f1_score(y_train, pred_train))\n",
    "display(create_2x2_matrix(y_train, pred_train))\n",
    "# print_2x2_miss_pred(y_train, pred_train,x_train).head()\n",
    "print('test')\n",
    "\n",
    "print(f1_score(y_test, pred_test))\n",
    "display(create_2x2_matrix(y_test, pred_test))\n",
    "# print_2x2_miss_pred(true_y=y_test, pred_y=pred_test,df=x_test).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# 1対その他での学習\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "# def vectorize(x_train,x_test, vec):\n",
    "#     x_train_vec = vec.fit_transform(x_train.content)\n",
    "#     x_test_vec = vec.transform(x_test.content)\n",
    "    \n",
    "#     x_train_vec = pd.DataFrame(x_train_vec.todense())\n",
    "#     x_test_vec = pd.DataFrame(x_test_vec.todense())\n",
    "    \n",
    "#     x_train_vec[\"comment_id\"]=x_train.comment_id.values\n",
    "#     x_test_vec[\"comment_id\"]=x_test.comment_id.values\n",
    "    \n",
    "#     return x_train_vec,x_test_vec\n",
    "\n",
    "\n",
    "\n",
    "# def train_and_predict(df):\n",
    "\n",
    "#     X = df.drop(\"target\",axis=1)\n",
    "#     y = df.target\n",
    "\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "    \n",
    "#     vec = TfidfVectorizer()\n",
    "#     x_train_vec, x_test_vec = vectorize(x_train, x_test, vec)\n",
    "#     train_content = pd.DataFrame(x_train.content.values, columns=[\"content\"])\n",
    "#     test_content  = pd.DataFrame(x_test.content.values, columns=[\"content\"])\n",
    "#     x_train.drop([\"content\"],axis=1,inplace=True)\n",
    "#     x_test.drop([\"content\"],axis=1,inplace=True)\n",
    "#     new_x_train = x_train.merge(x_train_vec, on=\"comment_id\")#.drop('comment_id',axis=1)\n",
    "#     new_x_test  = x_test.merge(x_test_vec, on=\"comment_id\")#.drop('comment_id',axis=1)\n",
    "#     new_x_train.drop('comment_id', axis=1, inplace=True)\n",
    "#     new_x_test.drop('comment_id', axis=1, inplace=True)\n",
    "#     x_train.drop([\"comment_id\"],axis=1,inplace=True)\n",
    "#     x_test.drop([\"comment_id\"],axis=1,inplace=True)\n",
    "\n",
    "#     svm = SVC(C=1.0, random_state=42)\n",
    "#     vec_model = OneVsRestClassifier(svm)\n",
    "#     vec_model.fit(new_x_train, y_train)\n",
    "#     model = OneVsRestClassifier(svm)\n",
    "#     model.fit(x_train, y_train)\n",
    "#     print('fin')\n",
    "\n",
    "#     return {\n",
    "#         \"pred_train_with_vec\":  vec_model.predict(new_x_train),\n",
    "#         \"pred_test_with_vec\": vec_model.predict(new_x_test),\n",
    "#         \"pred_train\":model.predict(x_train),\n",
    "#         \"pred_test\":model.predict(x_test),\n",
    "#         \"x_train\":x_train,\n",
    "#         \"new_x_train\":new_x_train,\n",
    "#         \"y_train\":y_train,\n",
    "#         \"new_x_test\":new_x_test,\n",
    "#         \"x_test\":x_test,\n",
    "#         \"y_test\":y_test,\n",
    "#         \"train_content\":train_content,\n",
    "#         \"test_content\":test_content,\n",
    "# #         \"x_train_vec\":x_train_vec,\n",
    "# #         \"x_test_vec\":x_test_vec\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import recall_score\n",
    "# from sklearn.metrics import precision_score\n",
    "# import numpy as np\n",
    "# '''\n",
    "# mxm,ここでは3x3のマトリックスを作成する\n",
    "# そこからf1値を計算する（同ディレクトリの画像参照）\n",
    "# '''\n",
    "\n",
    "# def create_3x3_matrix(true_y, pred_y):\n",
    "#     matrix = np.zeros(9)\n",
    "\n",
    "#     for t,p in zip(true_y, pred_y):\n",
    "#         if t==0:\n",
    "#             if p==0:matrix[0]+=1\n",
    "#             if p==1:matrix[1]+=1\n",
    "#             if p==2:matrix[2]+=1\n",
    "#         elif t==1:\n",
    "#             if p==0:matrix[3]+=1\n",
    "#             if p==1:matrix[4]+=1\n",
    "#             if p==2:matrix[5]+=1\n",
    "#         elif t==2:\n",
    "#             if p==0:matrix[6]+=1\n",
    "#             if p==1:matrix[7]+=1\n",
    "#             if p==2:matrix[8]+=1\n",
    "\n",
    "#     return matrix.reshape(3,3)\n",
    "# # matrix=create_3x3_matrix(y_train, pred_train)\n",
    "# # matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv('../data/clean/mount_data.csv', index=False)\n",
    "# df1.to_excel('../data/clean/mount_data.xlsx', index=False)\n",
    "# df1 = pd.read_excel('../data/clean/mount_data.xlsx')\n",
    "# df1 = df1.dropna()\n",
    "# df1[\"type\"]=df1.target\n",
    "# df1 = make_bad_feature(df1, badwordlist=badwordlist, positivelist=positivelist, hinikulist=hinikulist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1[df1.target==0].shape[0], df1[df1.target==1].shape[0],df1[df1.target==2].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# predのキー\n",
    "# dict_keys(['pred_train', 'pred_test', 'x_train', 'new_x_train', 'y_train',\\\n",
    "# 'new_x_test', 'x_test', 'y_test', 'train_content', 'test_content'])\n",
    "# '''\n",
    "\n",
    "# def print_3x3miss_pred(pred, test=True, true_y=np.nan, pred_y=np.nan,df=np.nan):\n",
    "#     if test:\n",
    "#         true_y = pred[\"y_test\"]\n",
    "#         pred_y = pred[\"pred_test\"]\n",
    "#         df     = pred[\"test_content\"]\n",
    "#     else:\n",
    "#         true_y = pred[\"y_train\"]\n",
    "#         pred_y = pred[\"pred_train\"]\n",
    "#         df     = pred[\"train_content\"]\n",
    "\n",
    "\n",
    "#     idx_arr = []\n",
    "#     pred_judge=[]\n",
    "#     true_judge=[]\n",
    "#     for i in range(len(pred_y)):\n",
    "#         if pred_y[i]!=true_y.values[i]:\n",
    "# #             print(pred[i], true_y.values[i])\n",
    "#             idx_arr.append(i)\n",
    "#             pred_judge.append(pred_y[i])\n",
    "#             true_judge.append(true_y.values[i])\n",
    "\n",
    "#     def print_str_label(num):\n",
    "#         if num==0:return \"ニュートラル\"\n",
    "#         if num==1:return \"悪口\"\n",
    "#         if num==2:return \"皮肉\"\n",
    "#         return np.nan\n",
    "    \n",
    "#     col=[\"content\",\"true_label_id\",\"true_label\",\"pred_label_id\",\"pred_label\"]\n",
    "#     miss_data = pd.DataFrame(columns=col)\n",
    "#     for i,idx in enumerate(idx_arr):\n",
    "#         tmpdf = pd.DataFrame(np.array([df.iloc[idx].content, true_judge[i],\\\n",
    "#                                           print_str_label(true_judge[i]), pred_judge[i],\\\n",
    "#                                         print_str_label(pred_judge[i])]).reshape(1,-1),columns=col)\n",
    "#         miss_data = pd.concat([miss_data, tmpdf])\n",
    "        \n",
    "# #             display(df.iloc[idx].content)\n",
    "# #             print('true label  *',true_judge[i] , (print_str_label(true_judge[i])))\n",
    "# #             print('pred label  *',pred_judge[i], (print_str_label(pred_judge[i])))\n",
    "# #             print(\"------------------------------------------------------------------------------------\")\n",
    "        \n",
    "#     print(miss_data.shape)\n",
    "#     return miss_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "他のベクトル変換の例\n",
    "\n",
    "x_train,x_test, y_train, y_test = train_test_split(content, target, random_state=42)\n",
    "\n",
    "・バイナリ\n",
    "vec = CountVectorizer(binary=True)\n",
    "train_and_val(x_train, x_test, y_train, y_test, vec)\n",
    "\n",
    "・カウント\n",
    "print('count')\n",
    "vec = CountVectorizer(binary=False)\n",
    "train_and_val(x_train, x_test, y_train, y_test, vec)\n",
    "\n",
    "\n",
    "・TfIdf\n",
    "vec = TfidfVectorizer()\n",
    "vectorize(x_train, x_test, y_train, y_test, vec)\n",
    "\n",
    "・bigram\n",
    "vec = TfidfVectorizer(ngram_range=(1,3))\n",
    "train_and_val(x_train, x_test, y_train, y_test, vec)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = train_and_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"no vectorize train\")\n",
    "# matrix=create_3x3_matrix(pred[\"y_train\"], pred[\"pred_train\"])\n",
    "# print(matrix)\n",
    "# print(calc_scores(matrix))\n",
    "# print()\n",
    "# print()\n",
    "# print('no vectoizer test')\n",
    "# matrix=create_3x3_matrix(pred[\"y_test\"], pred[\"pred_test\"])\n",
    "# print(matrix)\n",
    "# print(calc_scores(matrix))\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# print(\"vectorize train\")\n",
    "# matrix=create_3x3_matrix(pred[\"y_train\"], pred[\"pred_train_with_vec\"])\n",
    "# print(matrix)\n",
    "# print(calc_scores(matrix))\n",
    "# print()\n",
    "# print()\n",
    "# print('vectorize test')\n",
    "# matrix=create_3x3_matrix(pred[\"y_test\"], pred[\"pred_test_with_vec\"])\n",
    "# print(matrix)\n",
    "# print(calc_scores(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6742897"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.wv[\"この\"]\n",
    "v2 = model.wv[\"殺す\"]\n",
    "\n",
    "cos_similarity(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
