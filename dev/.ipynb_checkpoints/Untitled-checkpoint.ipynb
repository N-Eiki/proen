{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "warnings.simplefilter('ignore')\n",
    "sys.path.append('/Users/nagataeiki/.pyenv/versions/3.7.4/lib/python3.7/site-packages')\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bad = pd.read_csv('../data/clean/cleaned_bad.csv')\n",
    "normal=pd.read_csv('../data/clean/cleaned_normal.csv')\n",
    "# hiniku_=pd.read_csv('../data/hiniku/hiniku_with_word.csv')\n",
    "hiniku = pd.read_csv(\"../data/clean/cleaned_hiniku.csv\")\n",
    "df = pd.read_csv('../data/clean/mount_all_data.csv')\n",
    "# df.drop('comment_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwordlist = pd.read_csv('../data/clean/mount_bad_list.csv')\n",
    "from janome.tokenizer import Tokenizer\n",
    "tokenizer=Tokenizer()\n",
    "badwordlist_reading = []\n",
    "for word  in badwordlist.bad.values:\n",
    "    text=\"\"\n",
    "    for t in tokenizer.tokenize(word):\n",
    "        if t.reading==\"*\":\n",
    "            pass\n",
    "        else:\n",
    "            text+=t.reading\n",
    "    if len(text)>=1:\n",
    "        badwordlist_reading.append(text)\n",
    "# pd.DataFrame(badwordlist_reading).to_csv('../data/clean/badwordlist_reading.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(res1, res2, res3,threshold=0.5,display=True):\n",
    "    train_ensemble = pd.DataFrame(np.array([res1[\"pred_train_un\"],res2[\"pred_train_un\"],res2[\"pred_train_un\"]]).sum(axis=0)/3,columns=[\"pred\"])\n",
    "    test_ensemble = pd.DataFrame(np.array([res1[\"pred_test_un\"],res2[\"pred_test_un\"],res2[\"pred_test_un\"]]).sum(axis=0)/3,columns=[\"pred\"])\n",
    "    train_ensemble = train_ensemble.pred.apply(lambda x: 1 if x>threshold else 0)\n",
    "    test_ensemble = test_ensemble.pred.apply(lambda x: 1 if x>threshold else 0)\n",
    "    train_score = f1_score(res1['y_train'], train_ensemble)\n",
    "    test_score = f1_score(res1['y_test'], test_ensemble)\n",
    "    \n",
    "    miss_pred_tr = print_2x2_miss_pred(true_y=res1[\"y_train_untreat\"], pred_y=train_ensemble, df=res1[\"x_train\"],label=\"train\")\n",
    "    miss_pred_te = print_2x2_miss_pred(true_y=res1[\"y_test_untreat\"], pred_y=test_ensemble, df=res1[\"x_test\"],label=\"test\")\n",
    "#     coll_pred_tr = print_2x2_collect_pred(true_y=res1[\"y_train_untreat\"], pred_y=train_ensemble, df=res1[\"x_train\"])\n",
    "#     coll_pred_te = print_2x2_collect_pred(true_y=res1[\"y_test_untreat\"], pred_y=test_ensemble, df=res1[\"x_test\"])\n",
    "    coll_pred_tr = print_2x2_collect_pred(true_y=res1[\"y_train\"], pred_y=train_ensemble, df=res1[\"x_train\"],label=\"train\")\n",
    "    coll_pred_te = print_2x2_collect_pred(true_y=res1[\"y_test\"], pred_y=test_ensemble, df=res1[\"x_test\"],label=\"test\")\n",
    "    \n",
    "    matrix_tr=np.array([coll_tr[coll_tr.pred_idx!=1].shape[0],\n",
    "               miss_tr[miss_tr.pred_idx==1].shape[0],\n",
    "               miss_tr[miss_tr.pred_idx!=1].shape[0], \n",
    "               coll_tr[coll_tr.pred_idx==1].shape[0]]).reshape(2,-1)\n",
    "    \n",
    "    matrix_te=np.array([coll_te[coll_te.pred_idx!=1].shape[0],\n",
    "           miss_te[miss_te.pred_idx==1].shape[0],\n",
    "           miss_te[miss_te.pred_idx!=1].shape[0], \n",
    "           coll_te[coll_te.pred_idx==1].shape[0]]).reshape(2,-1)\n",
    "    if display:\n",
    "        print(\"train:\")\n",
    "        print(train_score)\n",
    "        print(matrix_tr)\n",
    "        print('test:')\n",
    "        print(test_score)\n",
    "        print(matrix_te)\n",
    "    \n",
    "    return {\"train_score\":train_score,\n",
    "            \"test_score\":test_score,\n",
    "            \"pred_train\":train_ensemble,\n",
    "            \"pred_test\":test_ensemble,\n",
    "#             \"miss_train_df\":miss_pred_tr,\n",
    "#             \"miss_test_df\":miss_pred_te,\n",
    "#             \"collect_train_df\":coll_pred_tr,\n",
    "#             \"collect_test_df\":coll_pred_te,\n",
    "            \"matrix_tr\":matrix_tr,\n",
    "            \"matrix_te\":matrix_te,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_and_pred(df, threshold=0.5, debug=False, random_state=42,display=True):\n",
    "    df = df.dropna()\n",
    "    ids = df.id.values\n",
    "    X = df.drop([\"target\"],axis=1)#.drop('content', axis=1)\n",
    "    y=df[\"target\"]\n",
    "    x_train, x_test, y_train_untreat, y_test_untreat = train_test_split(X,y, test_size=0.2, random_state=random_state)    \n",
    "    \n",
    "    y_train = y_train_untreat.apply(lambda x: 1 if x==1 else 0)#悪口vs その他\n",
    "    y_test = y_test_untreat.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "    tr_id = x_train.id.values\n",
    "    x_train.drop('id', axis=1, inplace=True)\n",
    "    te_id = x_test.id.values\n",
    "    x_test.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    svm = SVR()\n",
    "    svm.fit(x_train.drop(['content'], axis=1), y_train)\n",
    "    pred_train_unadjust = svm.predict(x_train.drop(['content'], axis=1))\n",
    "    pred_test_unadjust = svm.predict(x_test.drop(['content'], axis=1))\n",
    "    pred_train = [1 if x>threshold else 0 for x in pred_train_unadjust]\n",
    "    pred_test = [1 if x>threshold else 0 for x in pred_test_unadjust]\n",
    "    tr_f = f1_score(y_train, pred_train)\n",
    "    te_f = f1_score(y_test, pred_test)\n",
    "    if display:\n",
    "        print(\"train\")\n",
    "        print(\"f1: \" ,tr_f)\n",
    "        print(create_2x2_matrix(y_train, pred_train))\n",
    "        # print_2x2_miss_pred(y_train, pred_train,x_train)\n",
    "        print('---------------------------------')\n",
    "        print('test')\n",
    "\n",
    "        print(\"f1: \",te_f)\n",
    "        print(create_2x2_matrix(y_test, pred_test))\n",
    "        print('---------------------------------')\n",
    "        print('---------------------------------')\n",
    "    train_miss = print_2x2_miss_pred(true_y=y_train_untreat,pred_y=pred_train, label=\"train\",df=x_train)\n",
    "    test_miss =  print_2x2_miss_pred(true_y=y_test_untreat, pred_y=pred_test, label=\"test\",df=x_test)\n",
    "    train_coll = print_2x2_collect_pred(true_y=y_train_untreat,pred_y=pred_train, label=\"train\",df=x_train)\n",
    "    test_coll = print_2x2_collect_pred(true_y=y_test_untreat, pred_y=pred_test, label=\"test\",df=x_test)\n",
    "    df[\"id\"]= ids\n",
    "    return {\n",
    "        \"miss_train_df\":train_miss, \n",
    "        \"miss_test_df\":test_miss,\n",
    "        \"collect_train_df\":train_coll,\n",
    "        \"collect_test_df\":test_coll,\n",
    "        \"x_train\":x_train,\n",
    "        \"y_train\":y_train,\n",
    "        \"x_test\":x_test,\n",
    "        \"y_test\":y_test,\n",
    "        \"pred_train\":pred_train,\n",
    "        \"pred_test\":pred_test,\n",
    "        \"model\":svm,\n",
    "        \"pred_train_un\":pred_train_unadjust,\n",
    "        \"pred_test_un\":pred_test_unadjust,\n",
    "        \"df\":df,\n",
    "        \"train_score\":tr_f,\n",
    "        \"test_score\":te_f,\n",
    "        \"y_train_untreat\":y_train_untreat,\n",
    "        \"y_test_untreat\":y_test_untreat\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2x2_matrix(true_y, pred_y):\n",
    "    matrix = np.zeros(4)\n",
    "\n",
    "    for t,p in zip(true_y, pred_y):\n",
    "        if t==0:\n",
    "            if p==0:matrix[0]+=1\n",
    "            if p==1:matrix[1]+=1\n",
    "        elif t==1:\n",
    "            if p==0:matrix[2]+=1\n",
    "            if p==1:matrix[3]+=1\n",
    "    return matrix.reshape(2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_2x2_miss_pred(true_y, pred_y,df, label,test=True):\n",
    "    def print_str_label(num, phase=\"predict\"):\n",
    "        if phase==\"predict\":\n",
    "            if num==0:return \"その他\"\n",
    "            if num==1:return \"悪口\"\n",
    "            else:\n",
    "                return np.nan\n",
    "            \n",
    "        else:   \n",
    "            if num==0:return \"その他[普通]\"\n",
    "            if num==1:return \"悪口\"\n",
    "            if num==2:return \"その他[皮肉]\"\n",
    "            return np.nan\n",
    "\n",
    "    idx_arr = []\n",
    "    pred_judge=[]\n",
    "    true_judge=[]\n",
    "    col=[\"label\",\"content\",\"judge\", \"pred_idx\", \"pred_label\",\"true_idx\", \"true_label\"]\n",
    "    returndf = pd.DataFrame(columns=col)\n",
    "    for i in range(len(df)):\n",
    "        if pred_y[i]!=true_y.values[i] and abs(pred_y[i]-true_y.values[i])!=2:\n",
    "            tmpdf = pd.DataFrame(np.array([label,df.iloc[i].content,\"miss\", pred_y[i], print_str_label(pred_y[i],phase=\"predict\"),\\\n",
    "                                            true_y.values[i], print_str_label(true_y.values[i], phase=\"\")]).reshape(1,-1), \\\n",
    "                                              columns=col)\n",
    "            returndf = pd.concat([returndf, tmpdf])\n",
    "    return returndf\n",
    "\n",
    "\n",
    "def print_2x2_collect_pred(true_y, pred_y,df,label, test=True):\n",
    "    def print_str_label(num, phase):\n",
    "        \n",
    "        if phase==\"predict\":\n",
    "            if num==0:return \"その他\"\n",
    "            if num==1:return \"悪口\"\n",
    "            else:\n",
    "                return np.nan\n",
    "            \n",
    "        else:   \n",
    "            if num==0:return \"その他[普通]\"\n",
    "            if num==1:return \"悪口\"\n",
    "            if num==2:return \"その他[皮肉]\"\n",
    "            return np.nan\n",
    "        \n",
    "    idx_arr = []\n",
    "    pred_judge=[]\n",
    "    true_judge=[]\n",
    "    col=[\"label\",\"content\", \"judge\",\"pred_idx\", \"pred_label\",\"true_idx\", \"true_label\"]\n",
    "    returndf = pd.DataFrame(columns=col)\n",
    "    for i in range(len(df)):\n",
    "        if pred_y[i]==true_y.values[i]:\n",
    "            tmpdf = pd.DataFrame(np.array([label,df.iloc[i].content,\"collect\", pred_y[i], print_str_label(pred_y[i],phase=\"predict\"),\\\n",
    "                                            true_y.values[i], print_str_label(true_y.values[i], phase=\"\")]).reshape(1,-1), \\\n",
    "                                              columns=col)\n",
    "            returndf = pd.concat([returndf, tmpdf])\n",
    "    return returndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wakati_content(text):\n",
    "    wakati = []\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        wakati.append(t.surface)\n",
    "    return wakati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "# # data = pd.read_csv('../data/clean/mount_bad_list.csv')\n",
    "# data = pd.read_csv(\"../data/clean/mount_all_data.csv\")\n",
    "# data = data[data.type==1]\n",
    "wakati_arr = []\n",
    "# for r in tqdm(data.iterrows()):\n",
    "#     wakati = create_wakati_content(r[1].content)\n",
    "#     wakati_arr.append(wakati)\n",
    "# model = Word2Vec(wakati_arr, sg=1, size=100, window=5, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec.load(\"./word2vecModel/100000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "悪口ワードの含有\n",
    "皮肉ワードの含有\n",
    "ポジティブワードの含有\n",
    "\n",
    "[形態素解析した時の単語数],[単純に悪口単語が文章に含まれているのかのフラグ], [形態素ごとに現れる悪口数]\n",
    "[皮肉単語が含まれているかのフラグ],[形態素ごとに現れる皮肉数]\n",
    "[ポジティブな単語が含まれているかのフラグ], [形態素ごとに現れるポジティブ単語数]\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "# df = bad.copy()\n",
    "# df = pd.concat([hiniku_, normal])\n",
    "# df = pd.concat([bad, df])\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from janome.tokenizer import Tokenizer\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "\n",
    "df = pd.read_csv('../data/clean/mount_all_data.csv')\n",
    "df.drop('comment_id', axis=1, inplace=True)\n",
    "tokenizer = Tokenizer()\n",
    "#悪口が含まれている数のカウント, 単に含まれているのかのフラグを返す\n",
    "def get_word_count(text, wordlist):\n",
    "    textlist = []\n",
    "    count=0\n",
    "    flg=0\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        if t.surface in wordlist:\n",
    "            count+=1\n",
    "    for w in wordlist:\n",
    "        if w in text:\n",
    "            flg=1\n",
    "            return count, flg\n",
    "    \n",
    "    return count,flg\n",
    "\n",
    "\n",
    "def get_order_label(text):\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        if t.infl_form[:2]==\"命令\":\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "'''\n",
    "###ここを実行することでword2vecのモデル作成\n",
    "from gensim.models import Word2Vec\n",
    "data = pd.read_csv('../data/some_comment_data.csv')\n",
    "newdata=pd.concat([pd.DataFrame(data.content), pd.DataFrame(df.content)])\n",
    "wakati_arr = []\n",
    "for r in tqdm(newdata.iterrows()):\n",
    "    wakati = create_wakati_content(r[1].content)\n",
    "    wakati_arr.append(wakadti)\n",
    "model = Word2Vec(wakati_arr, sg=1, size=100, window=5, min_count=1)\n",
    "'''\n",
    "\n",
    "def calc_cosine_similary(text,model,debug=False):\n",
    "    text_list = []\n",
    "    scores = {}\n",
    "\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        text_list.append(t.surface)\n",
    "    for word in text_list:\n",
    "        try:\n",
    "            vector1 = model.wv[word]\n",
    "            scores[word] = []\n",
    "            for bad in bad_list:\n",
    "                try:\n",
    "                    vector2 = model.wv[bad]\n",
    "                    score = cos_similarity(vector1,vector2)\n",
    "                    scores[word].append(score)\n",
    "                except :\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "    all_scores = []\n",
    "    for scores in scores.values():\n",
    "        all_scores +=scores\n",
    "    all_scores= np.array(all_scores)\n",
    "    typicalV = np.median(all_scores, axis=0)\n",
    "\n",
    "    if debug:\n",
    "        return scores\n",
    "    return typicalV\n",
    "\n",
    "\n",
    "def mophologicize(text):\n",
    "    words = []\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        words.append(t.surface)    \n",
    "    return words\n",
    "\n",
    "\n",
    "#dfと悪口辞書を入れて新しい特徴量を含んだdfを返す\n",
    "def make_features(df, badwordlist, positivelist=np.nan, hinikulist=np.nan):\n",
    "#     col=[\"content\",\"mophologics_num\", \"bad_per_mophologic\", \"simple_bad_flg\",\"target\", \"hiniku_per_mophologic\",\\\n",
    "#          \"simple_hiniku_flg\", \"positive_per_mophologic\", \"simple_positive_flg\"]\n",
    "#     import gensim\n",
    "#     PATH=\"./gensimmodel/word2vec.gensim.model\"\n",
    "#     model = gensim.models.Word2Vec.load(PATH)\n",
    "    col=[\"content\",\"mophologics_num\", \"bad_count_in_mophologic\", \"simple_bad_flg\",\"target\",\"order\",\"mophologic_content\"]\n",
    "\n",
    "    basedf = pd.DataFrame(columns=col)\n",
    "    tokenizer = Tokenizer()\n",
    "    for r in tqdm(df.iterrows()):\n",
    "        text = str(r[1].content)\n",
    "        target = r[1].type\n",
    "        num = len(tokenizer.tokenize(text))\n",
    "        badcount,badflg = get_word_count(text, badwordlist)\n",
    "        orderlabel = get_order_label(text)\n",
    "        mophologic = mophologicize(text)\n",
    "        newdf = pd.DataFrame(np.array([text, num, badcount,badflg,target, orderlabel,mophologic]).reshape(1,-1),columns=col)\n",
    "        basedf = pd.concat([basedf, newdf])\n",
    "\n",
    "    basedf[\"target\"] = basedf.apply(lambda x: int(x[\"target\"]), axis=1)\n",
    "#     basedf[\"comment_id\"] = basedf.index\n",
    "    basedf[\"bad_count_in_mophologic\"] = basedf.apply(lambda x: float(x[\"bad_count_in_mophologic\"]), axis=1)\n",
    "    basedf[\"mophologics_num\"] = basedf.apply(lambda x: float(x[\"mophologics_num\"]), axis=1)\n",
    "    basedf[\"order\"] = basedf.order.apply(lambda x: int(x))\n",
    "#     basedf[\"cosine_similar\"] = basedf.cosine_similar.apply(lambda x: float(x))\n",
    "    \n",
    "    return basedf\n",
    "# df =make_features(df, list(badwordlist.bad.values))\n",
    "# df = df.fillna(0.0)\n",
    "# # df = df[~df.duplicated(subset=\"content\")]\n",
    "# df = df.reset_index().drop('index', axis=1)\n",
    "# df[\"id\"] = df.index\n",
    "# display(df.head(2))\n",
    "# print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bad_word_list():\n",
    "    b_l=[]\n",
    "    #wor2vecの辞書にない単語の除外\n",
    "    for word in badwordlist.bad.values:\n",
    "        try:\n",
    "            a = model.wv[word]\n",
    "            b_l.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    return b_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['糞', '蛇足', 'ヘタ', 'ばっか', 'イタイ', 'KY', '死ね', 'ダサい', 'ゆとり', 'シカト',\n",
       "       '微妙', '死ねよ', '皆殺し', '悪い', 'バカ', 'いまいち', '駄目', 'もってこい', 'ひどい',\n",
       "       'キモス', '重い', '汚い', '嫉妬', 'キモイ', 'アホ', 'かわいそう', 'ヤラセ', '所詮', '無能',\n",
       "       'コケろ', 'ダメ', '愚民', '幼児', 'くたばれ', '馬鹿らしい', 'くだらない', '聞こえん', 'つまんない',\n",
       "       'くず', 'おもんない', 'へたくそ', 'カス', 'てめぇ', 'パクリ', '糞虫', 'ボケ', '自重',\n",
       "       'グダグダ', 'ばらばら', 'わるい', 'だから', '死', '馬鹿', 'しね', '恐ろしい', 'こわい',\n",
       "       'やめろ', '恐怖', '畏怖', '呪詛', '怖い', 'お祓い', 'ワガママ', 'オタク', '無情', 'ガイジ',\n",
       "       'くどい', '偽物', 'ダサ', 'クズ', 'ks', 'ごみ', 'ゴミ', 'クソ', '最低', 'ハゲ', '醜い',\n",
       "       'ポンコツ', '腹立つ', 'DQN', '殺', 'キモ', 'こいつ', 'ブサ', 'サイコ', 'ざこ', 'シスコン',\n",
       "       '池沼', 'ゴリラ', 'ロリコン', 'ころす', 'だっせ', 'うるさい', 'サイコパス', 'キチ', '下手糞',\n",
       "       'ガキ', '無駄', '下手くそ', 'オワコン', 'ホモ', '雑魚', '雑', 'よわ', 'チキン', 'ヲタ',\n",
       "       'ださ', '消せ', '変態', 'わる', '禿', 'つまんな', 'メンヘラ', 'BITCH', '酷い', '猿',\n",
       "       'ｶｽ', 'ひで', 'ざけんな', 'いやらしい', '不細工', 'くそ', 'きたない', '野郎', '黙れ', 'ザコ',\n",
       "       '辞め', '乞食', '弱い', 'つまらない', 'デブ', '狂', 'うんこ', 'ざっこ', '釣り', '下手',\n",
       "       'ウザ', 'ブス', '潰せ', 'へた', 'しょぼい', '人でなし', '屑', '嫌い', 'gm', 'きらい',\n",
       "       'クソムシ', 'クソゲー', 'きしょい', 'バカー', 'キモオタ', 'はげ'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('word_judge/judge_text/b_l.csv').T.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_vector_data(df):\n",
    "    col=[]\n",
    "    for i in range(100):\n",
    "\n",
    "        col.append(\"f_\"+str(i))\n",
    "\n",
    "    sandbox=pd.DataFrame(columns=col)\n",
    "    for r in tqdm(df.iterrows()):\n",
    "        text = r[1].mophologic_content\n",
    "        vec_list=[]\n",
    "        for t in text:\n",
    "                try:\n",
    "                    vec = model.wv[t]\n",
    "                    vec_list.append(vec)\n",
    "                except:\n",
    "                    vec_list.append(np.zeros(100))\n",
    "        vectors = pd.DataFrame(np.array(vec_list), columns=col)\n",
    "        tmp = pd.DataFrame(pd.DataFrame(vectors, columns=col).mean(axis=0)).T\n",
    "        sandbox = pd.concat([sandbox, tmp])\n",
    "        \n",
    "\n",
    "    sandbox.insert(0, \"target\",df.target.values)\n",
    "    sandbox.insert(1,\"content\",df.content.values)\n",
    "    sandbox[\"id\"]= df.id.values\n",
    "    return sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_cosine_sim_data(df):\n",
    "    b_l=create_bad_word_list()\n",
    "    base = pd.DataFrame(columns=np.array(b_l))\n",
    "    \n",
    "    for r in tqdm(df.iterrows()):\n",
    "        text = r[1].mophologic_content\n",
    "        ave_list=[]\n",
    "\n",
    "        for word in b_l:\n",
    "            try:\n",
    "                v1 = model.wv[word]\n",
    "                cos_list = []\n",
    "                for t in text:\n",
    "                    try:\n",
    "                        v2=model.wv[t]\n",
    "                        cos_list.append(cos_similarity(v1,v2))\n",
    "                        if word not in b_l:\n",
    "                            b_l.append(word)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            ave_list.append(np.array(cos_list).mean())\n",
    "        tmp=pd.DataFrame(np.array(ave_list).reshape(1,-1), columns=b_l)\n",
    "        base=pd.concat([base, tmp])\n",
    "        \n",
    "    base[\"target\"] =df.target.values\n",
    "    base[\"content\"] = df.content.values\n",
    "    base = base.loc[:,base.columns[::-1]]\n",
    "    base[\"id\"] = df.id.values\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_the_highest_cosine_sim_data(df):\n",
    "    b_l = create_bad_word_list()\n",
    "    base = pd.DataFrame(columns=np.array(b_l))\n",
    "    \n",
    "    for r in tqdm(df.iterrows()):\n",
    "        text = r[1].mophologic_content\n",
    "        highest_list=[]\n",
    "\n",
    "        for word in b_l:\n",
    "            try:\n",
    "                v1 = model.wv[word]\n",
    "                cos_list = []\n",
    "                for t in text:\n",
    "                    try:\n",
    "                        v2=model.wv[t]\n",
    "                        cos_list.append(cos_similarity(v1,v2))\n",
    "                        if word not in b_l:\n",
    "                            b_l.append(word)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                highest_list.append(np.array(cos_list).max())\n",
    "            except:\n",
    "                highest_list.append(np.nan)\n",
    "        tmp=pd.DataFrame(np.array(highest_list).reshape(1,-1), columns=b_l)\n",
    "        base=pd.concat([base, tmp])\n",
    "        \n",
    "    base[\"target\"] =df.target.values\n",
    "    base[\"content\"] = df.content.values\n",
    "    base = base.loc[:,base.columns[::-1]]\n",
    "    base[\"id\"] = df.id.values\n",
    "    return base\n",
    "\n",
    "\n",
    "# high_sim_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1929it [00:06, 303.51it/s]\n"
     ]
    }
   ],
   "source": [
    "df1 = create_mean_vector_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "f1:  0.7585743106926699\n",
      "[[620. 175.]\n",
      " [184. 564.]]\n",
      "---------------------------------\n",
      "test\n",
      "f1:  0.6704225352112677\n",
      "[[150.  52.]\n",
      " [ 65. 119.]]\n",
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "res1 = train_and_pred(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1929it [00:50, 37.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "f1:  0.636429085673146\n",
      "[[545. 209.]\n",
      " [296. 442.]]\n",
      "---------------------------------\n",
      "test\n",
      "f1:  0.6382978723404256\n",
      "[[118.  62.]\n",
      " [ 74. 120.]]\n",
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "df2 = create_mean_cosine_sim_data(df)\n",
    "res2 = train_and_pred(df2,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1929it [00:54, 35.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "f1:  0.6666666666666666\n",
      "[[566. 188.]\n",
      " [275. 463.]]\n",
      "---------------------------------\n",
      "test\n",
      "f1:  0.6892655367231638\n",
      "[[142.  38.]\n",
      " [ 72. 122.]]\n",
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "df3 = create_the_highest_cosine_sim_data(df)\n",
    "res3 = train_and_pred(df3,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1929, 103), (1929, 155), (1929, 155))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df1.shape, df2.shape, df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1929, 103), (1866, 155), (1866, 155))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res1[\"df\"].shape,res2[\"df\"].shape,res3[\"df\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_id = list(res2[\"df\"].id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1929/1929 [00:09<00:00, 202.95it/s]\n"
     ]
    }
   ],
   "source": [
    "newdf = pd.DataFrame(columns=df.columns)\n",
    "for i in tqdm(range(len(df))):\n",
    "    if i in valid_id:\n",
    "        tmp = pd.DataFrame(df.iloc[i]).T\n",
    "        newdf = pd.concat([newdf, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1866it [00:10, 171.56it/s]\n",
      "1866it [01:13, 25.53it/s]\n",
      "1866it [01:01, 30.21it/s]\n"
     ]
    }
   ],
   "source": [
    "df1=create_mean_vector_data(newdf)\n",
    "df2=create_the_highest_cosine_sim_data(newdf)\n",
    "df3=create_mean_cosine_sim_data(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "f1:  0.7496598639455783\n",
      "[[573. 181.]\n",
      " [187. 551.]]\n",
      "---------------------------------\n",
      "test\n",
      "f1:  0.723404255319149\n",
      "[[134.  46.]\n",
      " [ 58. 136.]]\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "train\n",
      "f1:  0.6671511627906976\n",
      "[[575. 179.]\n",
      " [279. 459.]]\n",
      "---------------------------------\n",
      "test\n",
      "f1:  0.670487106017192\n",
      "[[142.  38.]\n",
      " [ 77. 117.]]\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "train\n",
      "f1:  0.6372832369942196\n",
      "[[549. 205.]\n",
      " [297. 441.]]\n",
      "---------------------------------\n",
      "test\n",
      "f1:  0.6469002695417789\n",
      "[[123.  57.]\n",
      " [ 74. 120.]]\n",
      "---------------------------------\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "res1 = train_and_pred(df1)\n",
    "res2 = train_and_pred(df2)\n",
    "res3 = train_and_pred(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ens=ensemble(res1, res2, res3,threshold=0.5,display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def predict_new_data(text):\n",
    "    mophologics = mophologicize(text)\n",
    "    col=[\"content\", \"mophologic_content\"]\n",
    "    text_df=pd.DataFrame(np.array([text, mophologics]), ).T\n",
    "\n",
    "    text_df.columns = col\n",
    "    text_df[\"target\"]=np.nan\n",
    "    text_df[\"id\"]=np.nan\n",
    "    val1=create_mean_vector_data(text_df).drop(['id',\"content\",\"target\"], axis=1)\n",
    "    val2=create_the_highest_cosine_sim_data(text_df).drop(['id',\"content\",\"target\"], axis=1)\n",
    "    val3=create_mean_cosine_sim_data(text_df).drop(['id',\"content\",\"target\"], axis=1)\n",
    "#     display(val1)\n",
    "#     display(val1)\n",
    "#     display(val1)\n",
    "    val1.to_csv('./word_judge/judge_text/abc.csv', index=False)\n",
    "    m1 = pickle.load(open('./models/svm_1.sav',\"rb\"))\n",
    "    m2 = pickle.load(open('./models/svm_2.sav',\"rb\"))\n",
    "    m3 = pickle.load(open('./models/svm_3.sav',\"rb\"))\n",
    "#     return val2\n",
    "    p_1=m1.predict(val1)\n",
    "    p_2=m2.predict(val2)\n",
    "    p_3=m3.predict(val3)\n",
    "    return{\n",
    "        \"1\":p_1,\n",
    "        \"2\":p_2,\n",
    "        \"3\":p_3,\n",
    "        \"悪口度\":(p_1+p_2+p_3)/3\n",
    "    }\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(res1[\"model\"], open(\"./models/svm_1.sav\", \"wb\"))\n",
    "# pickle.dump(res2[\"model\"], open(\"./models/svm_2.sav\", \"wb\"))\n",
    "# pickle.dump(res3[\"model\"], open(\"./models/svm_3.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge():\n",
    "    text = input()\n",
    "    if len(text)<1:\n",
    "        print(\"文字数が少なすぎます\")\n",
    "    else:\n",
    "        print(predict_new_data(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " こんにちは\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 29.20it/s]\n",
      "1it [00:00, 26.31it/s]\n",
      "1it [00:00, 23.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': array([0.89251894]), '2': array([0.72497525]), '3': array([0.96502589]), '悪口度(f値)': array([0.86084002])}\n"
     ]
    }
   ],
   "source": [
    "judge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['こんにちは']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mophologicize(\"こんにちは\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_the_highest_cosine_sim_data(df):\n",
    "    b_l = create_bad_word_list()\n",
    "    base = pd.DataFrame(columns=np.array(b_l))\n",
    "    \n",
    "    for r in tqdm(df.iterrows()):\n",
    "        text = r[1].mophologic_content\n",
    "        highest_list=[]\n",
    "\n",
    "        for word in b_l:\n",
    "            try:\n",
    "                v1 = model.wv[word]\n",
    "                cos_list = []\n",
    "                for t in text:\n",
    "                    try:\n",
    "                        v2=model.wv[t]\n",
    "                        cos_list.append(cos_similarity(v1,v2))\n",
    "                        if word not in b_l:\n",
    "                            b_l.append(word)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                highest_list.append(np.array(cos_list).max())\n",
    "            except:\n",
    "                highest_list.append(np.nan)\n",
    "        tmp=pd.DataFrame(np.array(highest_list).reshape(1,-1), columns=b_l)\n",
    "        base=pd.concat([base, tmp])\n",
    "        \n",
    "    base[\"target\"] =df.target.values\n",
    "    base[\"content\"] = df.content.values\n",
    "    base = base.loc[:,base.columns[::-1]]\n",
    "    base[\"id\"] = df.id.values\n",
    "    return base\n",
    "\n",
    "\n",
    "# high_sim_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = train_and_pred(df1,)\n",
    "res2 = train_and_pred(df2,)\n",
    "res3 = train_and_pred(df3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "0.7402689313517339\n",
      "[[602 152]\n",
      " [215 523]]\n",
      "test:\n",
      "0.7645429362880886\n",
      "[[151  29]\n",
      " [ 56 138]]\n"
     ]
    }
   ],
   "source": [
    "res=ensemble(res1,res2,res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def change_seed(df1,df2,df3,show=False, iterations=5, start=0, result=False, ensembleflg=True):\n",
    "    trains=[]\n",
    "    tests =[]\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "            if ensembleflg:\n",
    "                res1 = train_and_pred(df1,display=show,random_state=i+start)\n",
    "                res2 = train_and_pred(df2,display=show,random_state=i+start)\n",
    "                res3 = train_and_pred(df3,display=show,random_state=i+start)\n",
    "                res=ensemble(res1,res2,res3,display=show)\n",
    "                trains.append(res[\"train_score\"])\n",
    "                tests.append(res[\"test_score\"])\n",
    "\n",
    "            else:\n",
    "                res1 = train_and_pred(df1,display=show,random_state=i+start)\n",
    "                trains.append(res1[\"train_score\"])\n",
    "                tests.append(res1[\"test_score\"])\n",
    "                \n",
    "                \n",
    "    trains = np.array(trains)\n",
    "    tests = np.array(tests)\n",
    "        \n",
    "    if result:\n",
    "        print(trains.min(), trains.max(), trains.mean(),trains.var(ddof=-1))\n",
    "        print(tests.min(), tests.max(), tests.mean(),tests.var(ddof=-1))\n",
    "    return trains, tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:09<00:00, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7482014388489209 0.7622377622377622 0.7538380592421191 2.6788296712863618e-05\n",
      "0.7195467422096318 0.7507002801120449 0.7312533184467686 0.00010136488429915052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tr, te = change_seed(df1, df2, df3,start=1,result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:14<00:00,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743099787685775 0.7576791808873721 0.7531704141207362 2.2073483978668398e-05\n",
      "0.6875000000000001 0.7519582245430809 0.7207812165078652 0.000551265428313297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tr, te = change_seed(df1, df2, df3,start=1,result=True, ensembleflg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.010068012927045263, 0.023479042321042334)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.00010136488429915052),np.sqrt(te.var(ddof=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_tr=res[\"miss_train_df\"]\n",
    "miss_te=res[\"miss_test_df\"]\n",
    "coll_tr=res[\"collect_train_df\"]\n",
    "coll_te=res[\"collect_test_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_tr.to_csv('./predicted_data/miss_pred_train.csv', index=False)\n",
    "miss_te.to_csv('./predicted_data/miss_pred_test.csv', index=False)\n",
    "coll_tr.to_csv('./predicted_data/collect_pred_train.csv', index=False)\n",
    "coll_te.to_csv(\"./predicted_data/collect_pred_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[602 152]\n",
      " [215 523]]\n",
      "[[151  29]\n",
      " [ 56 138]]\n"
     ]
    }
   ],
   "source": [
    "print(res[\"matrix_tr\"])\n",
    "print(res[\"matrix_te\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #悪口と誤判定してしまった\n",
    "# res[\"miss_train_df\"].true_idx.value_counts()#普通 : 皮肉 = 83 ; 59\n",
    "# tmp = res[\"miss_train_df\"][res[\"miss_train_df\"].pred_idx==\"1\"]#\n",
    "# print(tmp[tmp.true_idx==\"2\"].shape)#悪口と判定された皮肉\n",
    "# tmp[tmp.true_idx==\"2\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train 正しく悪口\n",
    "普通に悪口のやつ\n",
    "523/675\n",
    "\n",
    "--------------------------------\n",
    "train 正しくその他\n",
    "602/817\n",
    "\n",
    "--------------------------------\n",
    "train その他を誤って悪口\n",
    "その他[普通]    83/695\n",
    "その他[皮肉]    59/59\n",
    "\n",
    "--------------------------------\n",
    "train 悪口を誤ってその他\n",
    "209/738\n",
    "\n",
    "〇〇だけどxx(逆説などに弱い？？)\n",
    "直接的だが似たような音の言葉で代替している単語はあまり識別できていないように感じる\n",
    "例：　クズ-9ず, 死ね-氏ね\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "61(全皮肉\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testデータについて\n",
    "\n",
    "ミスデータ\n",
    "1    56\n",
    "0    22\n",
    "2     7\n",
    "--------\n",
    "正解データ\n",
    "0    151\n",
    "1    138\n",
    "--------\n",
    "\n",
    "test 正しく悪口\n",
    "普通に悪口のやつ\n",
    "138/167\n",
    "\n",
    "--------------------------------\n",
    "test 正しくその他\n",
    "151/207\n",
    "\n",
    "--------------------------------\n",
    "test その他を誤って悪口\n",
    "29/207\n",
    "\n",
    "22(その他の割合\n",
    "\n",
    "?, !, ..., www, ぁぁぁ,〜 などの特殊な形の文末を持つ文章が多い？\n",
    "\n",
    "--------------------------------\n",
    "test 悪口を誤ってその他\n",
    "56/207\n",
    "悪口辞書の文字と少し形の違う文字\n",
    "クズ-kz（全角）\n",
    "遅い-おっそ\n",
    "うるさい-うっさ\n",
    "\n",
    "１単語や２単語合わせただけの短い文章\n",
    "exp)おっそ, 何これ？, 灰かよ, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.concat([a,c])\n",
    "te = pd.concat([b,d])\n",
    "tr.to_csv('./predicted_data/train.csv',index=False)\n",
    "te.to_csv('./predicted_data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>judge</th>\n",
       "      <th>pred_idx</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>true_idx</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>自称名人まじカッコよくないからコメしないでくれw</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>誰だよとかエアプ丸出しで草恥ずかしくないの？</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>掛け声いらんわ</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>音もひどいしプレイも酷い良いのは声だけ</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>犯人の頭がいいことだけは分かった</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>お前らが生きてること自体ミスみたいなもんなのによく人のミス叩けるな</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>で、考える頭無いから爆発オチっていう安直なネタしかでてこない。</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>新鮮すぎる</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>あなたの服のセンスって流行に流されずに個性的ですね。</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>関係ない実況者の名前出すの寒いなぁ...</td>\n",
       "      <td>miss</td>\n",
       "      <td>1</td>\n",
       "      <td>悪口</td>\n",
       "      <td>2</td>\n",
       "      <td>その他[皮肉]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                            content judge  pred_idx pred_label  \\\n",
       "0   train           自称名人まじカッコよくないからコメしないでくれw  miss         1         悪口   \n",
       "0   train             誰だよとかエアプ丸出しで草恥ずかしくないの？  miss         1         悪口   \n",
       "0   train                            掛け声いらんわ  miss         1         悪口   \n",
       "0   train                音もひどいしプレイも酷い良いのは声だけ  miss         1         悪口   \n",
       "0   train                   犯人の頭がいいことだけは分かった  miss         1         悪口   \n",
       "..    ...                                ...   ...       ...        ...   \n",
       "0   train  お前らが生きてること自体ミスみたいなもんなのによく人のミス叩けるな  miss         1         悪口   \n",
       "0   train    で、考える頭無いから爆発オチっていう安直なネタしかでてこない。  miss         1         悪口   \n",
       "0   train                              新鮮すぎる  miss         1         悪口   \n",
       "0   train         あなたの服のセンスって流行に流されずに個性的ですね。  miss         1         悪口   \n",
       "0   train               関係ない実況者の名前出すの寒いなぁ...  miss         1         悪口   \n",
       "\n",
       "   true_idx true_label  \n",
       "0         2    その他[皮肉]  \n",
       "0         2    その他[皮肉]  \n",
       "0         2    その他[皮肉]  \n",
       "0         2    その他[皮肉]  \n",
       "0         2    その他[皮肉]  \n",
       "..      ...        ...  \n",
       "0         2    その他[皮肉]  \n",
       "0         2    その他[皮肉]  \n",
       "0         2    その他[皮肉]  \n",
       "0         2    その他[皮肉]  \n",
       "0         2    その他[皮肉]  \n",
       "\n",
       "[61 rows x 7 columns]"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a.true_idx==\"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=create_mean_vector_data(newdf)\n",
    "df2=create_the_highest_cosine_sim_data(newdf)\n",
    "df3=create_mean_cosine_sim_data(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[\"model\"].predict()\n",
    "res2[\"model\"].predict()\n",
    "res3[\"model\"].predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = res[\"miss_train_df\"]\n",
    "b = res[\"miss_test_df\"]\n",
    "c = res[\"collect_train_df\"]\n",
    "d = res[\"collect_test_df\"]\n",
    "\n",
    "# train_df = pd.concat([a,c])\n",
    "# test_df = pd.concat([b,d])\n",
    "\n",
    "# predict_df = pd.concat([a,b,c,d])\n",
    "# train_df.to_csv('./predicted_data/train.csv', index=False)\n",
    "# test_df.to_csv('./predicted_data/test.csv',index=False)\n",
    "# predict_df.to_csv(\"./predicted_data/merged_train_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdf = base.copy()\n",
    "# testdf = testdf.dropna()\n",
    "# # testdf.target = testdf.target.apply(lambda x: 0 if x==0 else 1)#x==0(ニュートラル)の時0, その他は1\n",
    "# testdf.target = testdf.target.apply(lambda x: 0 if x==1 else 1)#x==1(悪口)の時0, その他は1 [0:悪口, 1:その他]\n",
    "# # testdf.target = testdf.target.apply(lambda x: 0 if x==2 else 1)#x==2(皮肉)の時0, その他は1\n",
    "# testdf.target.unique()\n",
    "# testdf.target.unique()\n",
    "# X = testdf.drop(\"target\",axis=1)#.drop('content', axis=1)\n",
    "# y=testdf.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[602., 152.],\n",
       "       [215., 523.]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"matrix_tr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1866"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[0]+ b.shape[0]+ c.shape[0]+d.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1対その他での学習\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "def train_and_predict_one_vs_rest(df):\n",
    "    \n",
    "    X = df.drop([\"target\",\"id\"],axis=1)\n",
    "    y = df.target\n",
    "    \n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "#     return x_train, y_train\n",
    "\n",
    "    train_content = pd.DataFrame(x_train.content.values, columns=[\"content\"])\n",
    "    test_content  = pd.DataFrame(x_test.content.values, columns=[\"content\"])\n",
    "#     x_train.drop(,axis=1,inplace=True)\n",
    "#     x_test.drop([\"content\"],axis=1,inplace=True)\n",
    "    svm = SVC()\n",
    "    ovr = OneVsRestClassifier(svm)\n",
    "    ovr.fit(x_train.drop('content',axis=1), list(y_train.values))\n",
    "    \n",
    "    tr_pre = ovr.predict(x_train.drop('content',axis=1))\n",
    "    te_pre = ovr.predict(x_test.drop('content',axis=1))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return {\n",
    "#         \"pred_train_with_vec\":  vec_model.predict(new_x_train),\n",
    "#         \"pred_test_with_vec\": vec_model.predict(new_x_test),\n",
    "        \"pred_train\":ovr.predict(x_train.drop('content',axis=1)),\n",
    "        \"pred_test\":ovr.predict(x_test.drop('content',axis=1)),\n",
    "        \"x_train\":x_train,\n",
    "#         \"new_x_train\":new_x_train,\n",
    "        \"y_train\":y_train,\n",
    "#         \"new_x_test\":new_x_test,\n",
    "        \"x_test\":x_test,\n",
    "        \"y_test\":y_test,\n",
    "        \"train_content\":train_content,\n",
    "        \"test_content\":test_content,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "'''\n",
    "mxm,ここでは3x3のマトリックスを作成する\n",
    "そこからf1値を計算する（同ディレクトリの画像参照）\n",
    "'''\n",
    "\n",
    "def create_3x3_matrix(true_y, pred_y):\n",
    "    matrix = np.zeros(9)\n",
    "\n",
    "    for t,p in zip(true_y, pred_y):\n",
    "        t = int(t)\n",
    "        p = int(p)\n",
    "        if t==0:\n",
    "            if p==0:matrix[0]+=1\n",
    "            if p==1:matrix[1]+=1\n",
    "            if p==2:matrix[2]+=1\n",
    "        elif t==1:\n",
    "            if p==0:matrix[3]+=1\n",
    "            if p==1:matrix[4]+=1\n",
    "            if p==2:matrix[5]+=1\n",
    "        elif t==2:\n",
    "            if p==0:matrix[6]+=1\n",
    "            if p==1:matrix[7]+=1\n",
    "            if p==2:matrix[8]+=1\n",
    "\n",
    "    return matrix.reshape(3,3)\n",
    "# matrix=create_3x3_matrix(y_train, pred_train)\n",
    "# matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# matrix\n",
    "\n",
    "'''\n",
    "matrix = np.array([\n",
    "    [240,20, 40],\n",
    "    [5,180, 15],\n",
    "    [25, 5, 70]\n",
    "])\n",
    "動作例\n",
    "acc: 0.8000000000000002\n",
    "recall 0.7756458897922313\n",
    "f1: 0.7876347291657112\n",
    "'''\n",
    "def calc_3x3_matrix_f_score(matrix):\n",
    "\n",
    "    accs = []\n",
    "    recalls=[]\n",
    "    tate=matrix.sum(axis=0)#たて\n",
    "    yoko=matrix.sum(axis=1)#横\n",
    "    for i in range(3):\n",
    "        accs.append(matrix[i][i]/yoko[i])\n",
    "        recalls.append(matrix[i][i]/tate[i])\n",
    "    acc = np.mean(accs)\n",
    "    recall = np.mean(recalls)\n",
    "\n",
    "    f1 = 2*acc*recall/(acc+recall)\n",
    "    return {\n",
    "        \"acc\":acc,\n",
    "        \"recall\":recall,\n",
    "        \"f1\":f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1[df1.target==0].shape[0], df1[df1.target==1].shape[0],df1[df1.target==2].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "predのキー\n",
    "dict_keys(['pred_train', 'pred_test', 'x_train', 'new_x_train', 'y_train',\\\n",
    "'new_x_test', 'x_test', 'y_test', 'train_content', 'test_content'])\n",
    "'''\n",
    "\n",
    "def print_3x3miss_pred(pred, test=True, true_y=np.nan, pred_y=np.nan,df=np.nan):\n",
    "    if test:\n",
    "        true_y = pred[\"y_test\"]\n",
    "        pred_y = pred[\"pred_test\"]\n",
    "        df     = pred[\"test_content\"]\n",
    "    else:\n",
    "        true_y = pred[\"y_train\"]\n",
    "        pred_y = pred[\"pred_train\"]\n",
    "        df     = pred[\"train_content\"]\n",
    "\n",
    "\n",
    "    idx_arr = []\n",
    "    pred_judge=[]\n",
    "    true_judge=[]\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i]!=true_y.values[i]:\n",
    "#             print(pred[i], true_y.values[i])\n",
    "            idx_arr.append(i)\n",
    "            pred_judge.append(pred_y[i])\n",
    "            true_judge.append(true_y.values[i])\n",
    "\n",
    "    def print_str_label(num):\n",
    "        if num==0:return \"ニュートラル\"\n",
    "        if num==1:return \"悪口\"\n",
    "        if num==2:return \"皮肉\"\n",
    "        return np.nan\n",
    "    \n",
    "    col=[\"content\",\"true_label_id\",\"true_label\",\"pred_label_id\",\"pred_label\"]\n",
    "    miss_data = pd.DataFrame(columns=col)\n",
    "    for i,idx in enumerate(idx_arr):\n",
    "        tmpdf = pd.DataFrame(np.array([df.iloc[idx].content, true_judge[i],\\\n",
    "                                          print_str_label(true_judge[i]), pred_judge[i],\\\n",
    "                                        print_str_label(pred_judge[i])]).reshape(1,-1),columns=col)\n",
    "        miss_data = pd.concat([miss_data, tmpdf])\n",
    "        \n",
    "#             display(df.iloc[idx].content)\n",
    "#             print('true label  *',true_judge[i] , (print_str_label(true_judge[i])))\n",
    "#             print('pred label  *',pred_judge[i], (print_str_label(pred_judge[i])))\n",
    "#             print(\"------------------------------------------------------------------------------------\")\n",
    "        \n",
    "    print(miss_data.shape)\n",
    "    return miss_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 create_mean_vector_data\n",
    "#df2 create_mean_cosine_sim_data\n",
    "#df3 createcreate_the_highest_cosine_sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'acc': 0.4988334641742502, 'recall': 0.7800663055590945, 'f1': 0.6085280280702309}\n",
      "[[309. 244.   0.]\n",
      " [ 79. 659.   0.]\n",
      " [ 46. 146.   9.]]\n",
      "test: {'acc': 0.4705326460481099, 'recall': 0.5290123456790123, 'f1': 0.4980617798391572}\n",
      "[[ 70.  69.   1.]\n",
      " [ 20. 172.   2.]\n",
      " [ 10.  29.   1.]]\n"
     ]
    }
   ],
   "source": [
    "# OneVsRest= train_and_predict_one_vs_rest(df1)\n",
    "# matrix_tr=create_3x3_matrix(true_y=OneVsRest[\"y_train\"], pred_y=OneVsRest[\"pred_train\"])\n",
    "# matrix_te=create_3x3_matrix(true_y=OneVsRest[\"y_test\"], pred_y=OneVsRest[\"pred_test\"])\n",
    "# print(\"train:\",calc_3x3_matrix_f_score(matrix_tr))\n",
    "# print(matrix_tr)\n",
    "# print(\"test:\",calc_3x3_matrix_f_score(matrix_te))\n",
    "# print(matrix_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'acc': 0.44383922139402227, 'recall': nan, 'f1': nan}\n",
      "[[271. 282.   0.]\n",
      " [117. 621.   0.]\n",
      " [ 61. 140.   0.]]\n",
      "test: {'acc': 0.41605301914580267, 'recall': nan, 'f1': nan}\n",
      "[[ 60.  80.   0.]\n",
      " [ 35. 159.   0.]\n",
      " [ 13.  27.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# OneVsRest= train_and_predict_one_vs_rest(df2)\n",
    "# matrix_tr=create_3x3_matrix(true_y=OneVsRest[\"y_train\"], pred_y=OneVsRest[\"pred_train\"])\n",
    "# matrix_te=create_3x3_matrix(true_y=OneVsRest[\"y_test\"], pred_y=OneVsRest[\"pred_test\"])\n",
    "# print(\"train:\",calc_3x3_matrix_f_score(matrix_tr))\n",
    "# print(matrix_tr)\n",
    "# print(\"test:\",calc_3x3_matrix_f_score(matrix_te))\n",
    "# print(matrix_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'acc': 0.4185213394062634, 'recall': 0.5800970795763215, 'f1': 0.48623779036100034}\n",
      "[[214. 337.   2.]\n",
      " [108. 630.   0.]\n",
      " [ 50. 148.   3.]]\n",
      "test: {'acc': 0.3930289641629848, 'recall': nan, 'f1': nan}\n",
      "[[ 46.  94.   0.]\n",
      " [ 29. 165.   0.]\n",
      " [ 14.  26.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# OneVsRest= train_and_predict_one_vs_rest(df3)\n",
    "# matrix_tr=create_3x3_matrix(true_y=OneVsRest[\"y_train\"], pred_y=OneVsRest[\"pred_train\"])\n",
    "# matrix_te=create_3x3_matrix(true_y=OneVsRest[\"y_test\"], pred_y=OneVsRest[\"pred_test\"])\n",
    "# print(\"train:\",calc_3x3_matrix_f_score(matrix_tr))\n",
    "# print(matrix_tr)\n",
    "# print(\"test:\",calc_3x3_matrix_f_score(matrix_te))\n",
    "# print(matrix_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
